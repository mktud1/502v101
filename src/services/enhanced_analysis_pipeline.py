#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Enhanced Analysis Pipeline
Pipeline de an√°lise aprimorado com Gemini 2.5 Pro e fallback Groq
"""

import time
import logging
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Callable
from services.ai_manager import ai_manager
from services.production_search_manager import production_search_manager
from services.robust_content_extractor import robust_content_extractor
from services.content_synthesis_engine import content_synthesis_engine
from services.mental_drivers_architect import mental_drivers_architect
from services.visual_proofs_generator import visual_proofs_generator
from services.anti_objection_system import anti_objection_system
from services.enhanced_pre_pitch_architect import enhanced_pre_pitch_architect
from services.future_prediction_engine import future_prediction_engine
from services.local_file_manager import local_file_manager
from services.auto_save_manager import salvar_etapa, salvar_erro

logger = logging.getLogger(__name__)

class EnhancedAnalysisPipeline:
    """Pipeline de an√°lise aprimorado com IA avan√ßada e consolida√ß√£o robusta"""
    
    def __init__(self):
        """Inicializa pipeline aprimorado"""
        self.gemini_model = "gemini-2.0-flash-exp"  # Modelo mais avan√ßado
        self.groq_model = "llama-3.3-70b-versatile"  # Modelo Groq atualizado
        
        self.quality_thresholds = {
            'min_sources': 5,
            'min_content_length': 8000,
            'min_quality_score': 75.0,
            'min_insights': 20,
            'min_avatar_depth': 10
        }
        
        self.consolidation_rules = {
            'remove_raw_data': True,
            'enhance_insights': True,
            'validate_completeness': True,
            'ensure_local_backup': True,
            'generate_executive_summary': True
        }
        
        logger.info("Enhanced Analysis Pipeline inicializado com Gemini 2.5 Pro")
    
    def execute_complete_analysis(
        self, 
        data: Dict[str, Any],
        session_id: Optional[str] = None,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise completa aprimorada"""
        
        start_time = time.time()
        logger.info(f"üöÄ Iniciando an√°lise aprimorada para {data.get('segmento')}")
        
        try:
            # Fase 1: Pesquisa e S√≠ntese Inteligente
            if progress_callback:
                progress_callback(1, "üîç Executando pesquisa inteligente massiva...")
            
            research_data = self._execute_intelligent_research(data, progress_callback)
            
            # Fase 2: An√°lise com Gemini 2.5 Pro (fallback Groq)
            if progress_callback:
                progress_callback(3, "üß† Analisando com Gemini 2.5 Pro...")
            
            core_analysis = self._execute_advanced_ai_analysis(data, research_data)
            
            # Fase 3: Componentes Avan√ßados Robustos
            if progress_callback:
                progress_callback(5, "‚ö° Gerando componentes ultra-robustos...")
            
            advanced_components = self._generate_robust_components(core_analysis, data, progress_callback)
            
            # Fase 4: Consolida√ß√£o e Aprimoramento
            if progress_callback:
                progress_callback(10, "üìä Consolidando relat√≥rio final...")
            
            final_analysis = self._consolidate_enhanced_analysis(
                data, research_data, core_analysis, advanced_components
            )
            
            # Fase 5: Salvamento Local Garantido
            if progress_callback:
                progress_callback(12, "üíæ Salvando relat√≥rios localmente...")
            
            local_backup = self._ensure_local_backup(final_analysis, session_id)
            
            # Adiciona metadados finais
            final_analysis['metadata'] = self._generate_enhanced_metadata(
                start_time, research_data, core_analysis, advanced_components, local_backup
            )
            
            if progress_callback:
                progress_callback(13, "‚úÖ An√°lise aprimorada conclu√≠da!")
            
            logger.info(f"‚úÖ An√°lise aprimorada conclu√≠da em {time.time() - start_time:.2f}s")
            return final_analysis
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise aprimorada: {str(e)}")
            salvar_erro("pipeline_aprimorado", e, contexto=data)
            raise Exception(f"AN√ÅLISE APRIMORADA FALHOU: {str(e)}")
    
    def _execute_intelligent_research(
        self, 
        data: Dict[str, Any], 
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """Executa pesquisa inteligente com s√≠ntese avan√ßada"""
        
        logger.info("üîç Iniciando pesquisa inteligente massiva")
        
        # Gera queries inteligentes expandidas
        queries = self._generate_intelligent_queries(data)
        
        # Executa busca multi-fonte
        all_results = []
        extracted_content = []
        
        for i, query in enumerate(queries):
            if progress_callback:
                progress_callback(1, f"üîç Pesquisando: {query[:50]}...", f"Query {i+1}/{len(queries)}")
            
            try:
                # Busca com m√∫ltiplos provedores
                search_results = production_search_manager.search_with_fallback(query, max_results=12)
                all_results.extend(search_results)
                
                # Extrai conte√∫do com valida√ß√£o
                for result in search_results[:8]:
                    content = robust_content_extractor.extract_content(result['url'])
                    if content and len(content) > 500:
                        extracted_content.append({
                            'url': result['url'],
                            'title': result.get('title', ''),
                            'content': content,
                            'query': query,
                            'source': result.get('source', 'unknown')
                        })
                
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro na query '{query}': {e}")
                continue
        
        # S√≠ntese inteligente do conte√∫do
        if progress_callback:
            progress_callback(2, "üß† Sintetizando conte√∫do extra√≠do...")
        
        synthesis_result = content_synthesis_engine.synthesize_research_content(
            extracted_content, data
        )
        
        # Salva dados de pesquisa (sem conte√∫do bruto)
        research_summary = {
            'queries_executed': queries,
            'total_sources': len(extracted_content),
            'synthesis_result': synthesis_result,
            'statistics': {
                'total_queries': len(queries),
                'total_results': len(all_results),
                'successful_extractions': len(extracted_content),
                'total_content_length': sum(len(c['content']) for c in extracted_content),
                'unique_domains': len(set(c['url'].split('/')[2] for c in extracted_content)),
                'avg_content_length': sum(len(c['content']) for c in extracted_content) / len(extracted_content) if extracted_content else 0
            }
        }
        
        salvar_etapa("pesquisa_inteligente", research_summary, categoria="pesquisa_web")
        
        logger.info(f"‚úÖ Pesquisa inteligente: {len(extracted_content)} fontes, s√≠ntese com {len(synthesis_result.get('categorized_insights', {}).get('all_insights', []))} insights")
        
        return research_summary
    
    def _execute_advanced_ai_analysis(
        self, 
        data: Dict[str, Any], 
        research_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Executa an√°lise com Gemini 2.5 Pro (fallback Groq)"""
        
        logger.info("üß† Executando an√°lise com IA avan√ßada")
        
        # Prepara contexto de s√≠ntese
        synthesis_context = self._prepare_synthesis_context(research_data)
        
        # Constr√≥i prompt ultra-avan√ßado
        prompt = self._build_advanced_analysis_prompt(data, synthesis_context)
        
        # Tenta Gemini 2.5 Pro primeiro
        try:
            logger.info("üöÄ Tentando an√°lise com Gemini 2.5 Pro...")
            
            ai_response = ai_manager.generate_analysis(
                prompt, 
                max_tokens=8192,
                provider='gemini'  # For√ßa uso do Gemini
            )
            
            if ai_response:
                analysis = self._process_advanced_ai_response(ai_response, data, 'gemini')
                logger.info("‚úÖ An√°lise conclu√≠da com Gemini 2.5 Pro")
                return analysis
            else:
                raise Exception("Gemini n√£o retornou resposta v√°lida")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Gemini falhou: {e}")
            
            # Fallback para Groq
            try:
                logger.info("üîÑ Fallback para Groq...")
                
                ai_response = ai_manager.generate_analysis(
                    prompt,
                    max_tokens=8192,
                    provider='groq'  # For√ßa uso do Groq
                )
                
                if ai_response:
                    analysis = self._process_advanced_ai_response(ai_response, data, 'groq')
                    logger.info("‚úÖ An√°lise conclu√≠da com Groq (fallback)")
                    return analysis
                else:
                    raise Exception("Groq tamb√©m falhou")
                    
            except Exception as groq_error:
                logger.error(f"‚ùå Groq tamb√©m falhou: {groq_error}")
                raise Exception(f"AMBAS IAs FALHARAM: Gemini ({e}) | Groq ({groq_error})")
    
    def _generate_robust_components(
        self, 
        core_analysis: Dict[str, Any], 
        data: Dict[str, Any],
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """Gera componentes avan√ßados ultra-robustos"""
        
        components = {}
        
        # Drivers Mentais Ultra-Robustos
        if progress_callback:
            progress_callback(6, "üß† Gerando drivers mentais ultra-robustos...")
        
        try:
            avatar_data = core_analysis.get('avatar_ultra_detalhado', {})
            drivers_system = mental_drivers_architect.generate_complete_drivers_system(
                avatar_data, data
            )
            
            if drivers_system and not drivers_system.get('fallback_mode'):
                # Aprimora drivers com an√°lise adicional
                enhanced_drivers = self._enhance_mental_drivers(drivers_system, core_analysis)
                components['drivers_mentais_customizados'] = enhanced_drivers
                salvar_etapa("drivers_ultra_robustos", enhanced_drivers, categoria="drivers_mentais")
                logger.info("‚úÖ Drivers mentais ultra-robustos gerados")
            
        except Exception as e:
            logger.error(f"‚ùå Erro nos drivers mentais: {e}")
            salvar_erro("drivers_mentais", e)
        
        # Provas Visuais Inovadoras
        if progress_callback:
            progress_callback(7, "üé≠ Criando provas visuais inovadoras...")
        
        try:
            concepts = self._extract_proof_concepts(core_analysis, data)
            visual_proofs = visual_proofs_generator.generate_complete_proofs_system(
                concepts, avatar_data, data
            )
            
            if visual_proofs:
                # Aprimora provas com elementos inovadores
                enhanced_proofs = self._enhance_visual_proofs(visual_proofs, core_analysis)
                components['provas_visuais_inovadoras'] = enhanced_proofs
                salvar_etapa("provas_inovadoras", enhanced_proofs, categoria="provas_visuais")
                logger.info("‚úÖ Provas visuais inovadoras criadas")
            
        except Exception as e:
            logger.error(f"‚ùå Erro nas provas visuais: {e}")
            salvar_erro("provas_visuais", e)
        
        # Sistema Anti-Obje√ß√£o Avan√ßado
        if progress_callback:
            progress_callback(8, "üõ°Ô∏è Construindo sistema anti-obje√ß√£o avan√ßado...")
        
        try:
            objections = avatar_data.get('objecoes_reais', [])
            if not objections:
                objections = self._generate_intelligent_objections(core_analysis, data)
            
            anti_objection = anti_objection_system.generate_complete_anti_objection_system(
                objections, avatar_data, data
            )
            
            if anti_objection and not anti_objection.get('fallback_mode'):
                # Aprimora sistema com t√©cnicas avan√ßadas
                enhanced_anti_objection = self._enhance_anti_objection_system(anti_objection, core_analysis)
                components['sistema_anti_objecao_avancado'] = enhanced_anti_objection
                salvar_etapa("anti_objecao_avancado", enhanced_anti_objection, categoria="anti_objecao")
                logger.info("‚úÖ Sistema anti-obje√ß√£o avan√ßado constru√≠do")
            
        except Exception as e:
            logger.error(f"‚ùå Erro no sistema anti-obje√ß√£o: {e}")
            salvar_erro("anti_objecao", e)
        
        # Pr√©-Pitch Revolucion√°rio
        if progress_callback:
            progress_callback(9, "üéØ Arquitetando pr√©-pitch revolucion√°rio...")
        
        try:
            drivers_data = components.get('drivers_mentais_customizados', {})
            pre_pitch = enhanced_pre_pitch_architect.generate_enhanced_pre_pitch_system(
                drivers_data, avatar_data, data
            )
            
            if pre_pitch and pre_pitch.get('validacao_status') == 'ENHANCED_VALID':
                # Adiciona elementos revolucion√°rios
                revolutionary_pre_pitch = self._create_revolutionary_pre_pitch(pre_pitch, core_analysis)
                components['pre_pitch_revolucionario'] = revolutionary_pre_pitch
                salvar_etapa("pre_pitch_revolucionario", revolutionary_pre_pitch, categoria="pre_pitch")
                logger.info("‚úÖ Pr√©-pitch revolucion√°rio arquitetado")
            
        except Exception as e:
            logger.error(f"‚ùå Erro no pr√©-pitch: {e}")
            salvar_erro("pre_pitch", e)
        
        # Predi√ß√µes Futuras Avan√ßadas
        try:
            future_predictions = future_prediction_engine.predict_market_future(
                data.get('segmento', 'neg√≥cios'), data, horizon_months=48
            )
            
            if future_predictions:
                # Aprimora predi√ß√µes com an√°lise de cen√°rios
                enhanced_predictions = self._enhance_future_predictions(future_predictions, core_analysis)
                components['predicoes_futuro_avancadas'] = enhanced_predictions
                salvar_etapa("predicoes_avancadas", enhanced_predictions, categoria="predicoes_futuro")
                logger.info("‚úÖ Predi√ß√µes futuras avan√ßadas geradas")
            
        except Exception as e:
            logger.error(f"‚ùå Erro nas predi√ß√µes: {e}")
            salvar_erro("predicoes_futuro", e)
        
        return components
    
    def _consolidate_enhanced_analysis(
        self,
        data: Dict[str, Any],
        research_data: Dict[str, Any],
        core_analysis: Dict[str, Any],
        advanced_components: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Consolida an√°lise aprimorada sem dados brutos"""
        
        logger.info("üìä Consolidando an√°lise aprimorada")
        
        # Estrutura base consolidada
        consolidated = {
            'projeto_dados': data,
            'resumo_executivo': self._generate_executive_summary(core_analysis, advanced_components),
            'avatar_ultra_detalhado': self._enhance_avatar_data(core_analysis.get('avatar_ultra_detalhado', {})),
            'posicionamento_estrategico': self._enhance_positioning(core_analysis.get('escopo', {})),
            'analise_concorrencia_avancada': self._enhance_competition_analysis(core_analysis.get('analise_concorrencia_detalhada', [])),
            'estrategia_marketing_completa': self._enhance_marketing_strategy(core_analysis.get('estrategia_palavras_chave', {})),
            'metricas_kpis_avancados': self._enhance_metrics_analysis(core_analysis.get('metricas_performance_detalhadas', {})),
            'funil_vendas_detalhado': self._enhance_sales_funnel(core_analysis.get('funil_vendas_detalhado', {})),
            'plano_acao_detalhado': self._enhance_action_plan(core_analysis.get('plano_acao_detalhado', {})),
            'insights_exclusivos': self._enhance_insights(core_analysis.get('insights_exclusivos', [])),
            'inteligencia_mercado': self._generate_market_intelligence(research_data, core_analysis),
            'analise_oportunidades': self._generate_opportunity_analysis(core_analysis, advanced_components),
            'roadmap_implementacao': self._generate_implementation_roadmap(core_analysis, data)
        }
        
        # Adiciona componentes avan√ßados
        consolidated.update(advanced_components)
        
        # Remove dados brutos e adiciona apenas estat√≠sticas
        consolidated['pesquisa_web_massiva'] = {
            'estatisticas': research_data.get('statistics', {}),
            'qualidade_dados': 'PREMIUM - Dados reais validados',
            'fontes_consultadas': research_data.get('total_sources', 0),
            'insights_sintetizados': len(research_data.get('synthesis_result', {}).get('categorized_insights', {}).get('all_insights', []))
        }
        
        return consolidated
    
    def _ensure_local_backup(
        self, 
        analysis: Dict[str, Any], 
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Garante backup local completo"""
        
        logger.info("üíæ Garantindo backup local completo")
        
        try:
            # Salva an√°lise completa localmente
            backup_result = local_file_manager.save_analysis_locally(analysis)
            
            if backup_result['success']:
                logger.info(f"‚úÖ Backup local: {backup_result['total_files']} arquivos salvos")
                
                # Gera relat√≥rios em m√∫ltiplos formatos
                formats_generated = self._generate_multiple_formats(analysis, backup_result['analysis_id'])
                
                return {
                    'backup_successful': True,
                    'files_saved': backup_result['total_files'],
                    'analysis_id': backup_result['analysis_id'],
                    'base_directory': backup_result['base_directory'],
                    'formats_generated': formats_generated,
                    'backup_timestamp': datetime.now().isoformat()
                }
            else:
                logger.error(f"‚ùå Falha no backup local: {backup_result.get('error')}")
                return {'backup_successful': False, 'error': backup_result.get('error')}
                
        except Exception as e:
            logger.error(f"‚ùå Erro no backup local: {e}")
            return {'backup_successful': False, 'error': str(e)}
    
    def _generate_multiple_formats(self, analysis: Dict[str, Any], analysis_id: str) -> List[str]:
        """Gera relat√≥rios em m√∫ltiplos formatos"""
        
        formats = []
        
        try:
            import os
            from pathlib import Path
            
            # Diret√≥rio de relat√≥rios
            reports_dir = Path("relatorios_finais")
            reports_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            base_filename = f"analise_{analysis_id[:8]}_{timestamp}"
            
            # 1. Relat√≥rio Executivo (Markdown)
            executive_md = self._generate_executive_markdown(analysis)
            exec_path = reports_dir / f"{base_filename}_executivo.md"
            with open(exec_path, 'w', encoding='utf-8') as f:
                f.write(executive_md)
            formats.append(str(exec_path))
            
            # 2. Relat√≥rio T√©cnico (JSON estruturado)
            tech_json = self._generate_technical_json(analysis)
            tech_path = reports_dir / f"{base_filename}_tecnico.json"
            with open(tech_path, 'w', encoding='utf-8') as f:
                json.dump(tech_json, f, ensure_ascii=False, indent=2)
            formats.append(str(tech_path))
            
            # 3. Relat√≥rio de Implementa√ß√£o (Markdown)
            impl_md = self._generate_implementation_markdown(analysis)
            impl_path = reports_dir / f"{base_filename}_implementacao.md"
            with open(impl_path, 'w', encoding='utf-8') as f:
                f.write(impl_md)
            formats.append(str(impl_path))
            
            # 4. Dashboard de M√©tricas (HTML)
            dashboard_html = self._generate_metrics_dashboard(analysis)
            dash_path = reports_dir / f"{base_filename}_dashboard.html"
            with open(dash_path, 'w', encoding='utf-8') as f:
                f.write(dashboard_html)
            formats.append(str(dash_path))
            
            logger.info(f"‚úÖ {len(formats)} formatos de relat√≥rio gerados")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar formatos: {e}")
        
        return formats
    
    def _prepare_synthesis_context(self, research_data: Dict[str, Any]) -> str:
        """Prepara contexto de s√≠ntese para IA"""
        
        synthesis_result = research_data.get('synthesis_result', {})
        
        context = "S√çNTESE INTELIGENTE DE PESQUISA MASSIVA:\n\n"
        
        # Dados estruturados
        structured_data = synthesis_result.get('structured_data', {})
        if structured_data:
            context += "=== DADOS ESTRUTURADOS EXTRA√çDOS ===\n"
            for data_type, data_content in structured_data.items():
                context += f"{data_type.upper()}:\n"
                if isinstance(data_content, dict):
                    for key, values in data_content.items():
                        if isinstance(values, list) and values:
                            context += f"  {key}: {', '.join(str(v) for v in values[:5])}\n"
                context += "\n"
        
        # Insights categorizados
        insights = synthesis_result.get('categorized_insights', {})
        priority_insights = insights.get('priority_insights', [])
        if priority_insights:
            context += "=== INSIGHTS PRIORIT√ÅRIOS ===\n"
            for i, insight in enumerate(priority_insights[:15], 1):
                context += f"{i}. {insight}\n"
            context += "\n"
        
        # Padr√µes identificados
        patterns = synthesis_result.get('identified_patterns', {})
        if patterns:
            context += "=== PADR√ïES IDENTIFICADOS ===\n"
            
            themes = patterns.get('recurring_themes', [])
            if themes:
                context += f"Temas recorrentes: {', '.join(themes[:10])}\n"
            
            data_patterns = patterns.get('data_patterns', [])
            for pattern in data_patterns:
                context += f"Padr√£o: {pattern}\n"
            context += "\n"
        
        # Estat√≠sticas da pesquisa
        stats = research_data.get('statistics', {})
        context += "=== ESTAT√çSTICAS DA PESQUISA ===\n"
        context += f"Fontes analisadas: {stats.get('successful_extractions', 0)}\n"
        context += f"Conte√∫do total: {stats.get('total_content_length', 0):,} caracteres\n"
        context += f"Dom√≠nios √∫nicos: {stats.get('unique_domains', 0)}\n"
        context += f"Qualidade m√©dia: {stats.get('avg_content_length', 0):.0f} chars/fonte\n"
        
        return context
    
    def _build_advanced_analysis_prompt(self, data: Dict[str, Any], synthesis_context: str) -> str:
        """Constr√≥i prompt ultra-avan√ßado para IA"""
        
        return f"""# AN√ÅLISE ULTRA-AVAN√áADA - ARQV30 Enhanced v2.0

Voc√™ √© o DIRETOR SUPREMO DE AN√ÅLISE DE MERCADO com 30+ anos de experi√™ncia em consultoria estrat√©gica de elite.

## DADOS DO PROJETO:
- **Segmento**: {data.get('segmento', 'N√£o informado')}
- **Produto/Servi√ßo**: {data.get('produto', 'N√£o informado')}
- **P√∫blico-Alvo**: {data.get('publico', 'N√£o informado')}
- **Pre√ßo**: R$ {data.get('preco', 'N√£o informado')}
- **Objetivo de Receita**: R$ {data.get('objetivo_receita', 'N√£o informado')}
- **Or√ßamento Marketing**: R$ {data.get('orcamento_marketing', 'N√£o informado')}

{synthesis_context}

## MISS√ÉO CR√çTICA:
Gere uma an√°lise ULTRA-AVAN√áADA baseada na s√≠ntese inteligente acima. Cada se√ß√£o deve ter profundidade de consultoria de R$ 100.000/hora.

## FORMATO OBRIGAT√ìRIO:
```json
{{
  "avatar_ultra_detalhado": {{
    "nome_ficticio": "Nome representativo espec√≠fico",
    "perfil_demografico": {{
      "idade": "Faixa et√°ria espec√≠fica com dados precisos",
      "genero": "Distribui√ß√£o por g√™nero com percentuais",
      "renda": "Faixa de renda mensal espec√≠fica",
      "escolaridade": "N√≠vel educacional predominante",
      "localizacao": "Regi√µes geogr√°ficas espec√≠ficas",
      "estado_civil": "Status relacionamento predominante",
      "filhos": "Situa√ß√£o familiar t√≠pica",
      "profissao": "Ocupa√ß√µes espec√≠ficas mais comuns",
      "experiencia": "Anos de experi√™ncia no mercado",
      "tamanho_empresa": "Porte da empresa que trabalha"
    }},
    "perfil_psicografico": {{
      "personalidade": "Tra√ßos dominantes espec√≠ficos",
      "valores": "Valores e cren√ßas principais",
      "interesses": "Hobbies e interesses espec√≠ficos",
      "estilo_vida": "Como vive o dia a dia",
      "comportamento_compra": "Processo de decis√£o detalhado",
      "influenciadores": "Quem influencia decis√µes",
      "medos_profundos": "Medos relacionados ao neg√≥cio",
      "aspiracoes_secretas": "Aspira√ß√µes n√£o verbalizadas",
      "motivadores_internos": "O que realmente o motiva",
      "frustra√ß√µes_diarias": "Frustra√ß√µes do dia a dia"
    }},
    "dores_viscerais": [
      "Lista de 15-20 dores espec√≠ficas e viscerais"
    ],
    "desejos_secretos": [
      "Lista de 15-20 desejos profundos e espec√≠ficos"
    ],
    "objecoes_reais": [
      "Lista de 12-15 obje√ß√µes espec√≠ficas e reais"
    ],
    "jornada_emocional": {{
      "consciencia": "Como toma consci√™ncia do problema",
      "consideracao": "Processo de avalia√ß√£o de solu√ß√µes",
      "decisao": "Fatores decisivos para compra",
      "pos_compra": "Experi√™ncia p√≥s-compra esperada",
      "advocacy": "Como se torna promotor da solu√ß√£o"
    }},
    "linguagem_interna": {{
      "frases_dor": ["Frases espec√≠ficas que usa para dores"],
      "frases_desejo": ["Frases espec√≠ficas de desejo"],
      "metaforas_comuns": ["Met√°foras que usa"],
      "vocabulario_especifico": ["Jarg√µes do segmento"],
      "tom_comunicacao": "Tom preferido de comunica√ß√£o",
      "canais_preferidos": ["Canais de comunica√ß√£o preferidos"]
    }},
    "triggers_comportamentais": {{
      "gatilhos_compra": ["Gatilhos que levam √† compra"],
      "momentos_decisao": ["Momentos cr√≠ticos de decis√£o"],
      "influencias_externas": ["Fatores externos que influenciam"],
      "padroes_sazonais": ["Padr√µes sazonais de comportamento"]
    }}
  }},
  
  "escopo": {{
    "posicionamento_mercado": "Posicionamento √∫nico e espec√≠fico",
    "proposta_valor_unica": "Proposta irresist√≠vel espec√≠fica",
    "diferenciais_competitivos": [
      "Lista de diferenciais √∫nicos e defens√°veis"
    ],
    "mensagem_central": "Mensagem principal espec√≠fica",
    "tom_comunicacao": "Tom de voz ideal espec√≠fico",
    "nicho_especifico": "Nicho mais espec√≠fico recomendado",
    "estrategia_oceano_azul": "Como criar mercado sem concorr√™ncia",
    "ancoragem_preco": "Como ancorar pre√ßo na mente",
    "narrativa_marca": "Hist√≥ria da marca a ser contada",
    "missao_transformadora": "Miss√£o que transforma vidas"
  }},
  
  "analise_concorrencia_detalhada": [
    {{
      "nome": "Nome espec√≠fico do concorrente",
      "analise_swot": {{
        "forcas": ["For√ßas espec√≠ficas identificadas"],
        "fraquezas": ["Fraquezas espec√≠ficas explor√°veis"],
        "oportunidades": ["Oportunidades que eles n√£o veem"],
        "ameacas": ["Amea√ßas que representam"]
      }},
      "estrategia_marketing": "Estrat√©gia principal detalhada",
      "posicionamento": "Como se posicionam no mercado",
      "diferenciais": ["Principais diferenciais deles"],
      "vulnerabilidades": ["Pontos fracos explor√°veis"],
      "preco_estrategia": "Estrat√©gia de precifica√ß√£o",
      "share_mercado_estimado": "Participa√ß√£o estimada",
      "pontos_ataque": ["Onde atac√°-los estrategicamente"],
      "benchmarks": {{
        "preco": "Compara√ß√£o de pre√ßos",
        "qualidade": "Compara√ß√£o de qualidade",
        "atendimento": "Compara√ß√£o de atendimento",
        "inovacao": "N√≠vel de inova√ß√£o"
      }}
    }}
  ],
  
  "estrategia_palavras_chave": {{
    "palavras_primarias": [
      "20-25 palavras-chave principais espec√≠ficas"
    ],
    "palavras_secundarias": [
      "30-40 palavras-chave secund√°rias"
    ],
    "long_tail": [
      "40-60 palavras-chave de cauda longa"
    ],
    "intencao_busca": {{
      "informacional": ["Palavras para conte√∫do educativo"],
      "navegacional": ["Palavras para encontrar marca"],
      "transacional": ["Palavras para convers√£o"],
      "comercial": ["Palavras para compara√ß√£o"]
    }},
    "estrategia_conteudo": "Como usar palavras-chave estrategicamente",
    "sazonalidade": "Varia√ß√µes sazonais das buscas",
    "oportunidades_seo": "Oportunidades espec√≠ficas de SEO",
    "gaps_competitivos": ["Gaps de palavras-chave dos concorrentes"],
    "tendencias_busca": ["Tend√™ncias emergentes de busca"]
  }},
  
  "metricas_performance_detalhadas": {{
    "kpis_principais": [
      {{
        "metrica": "Nome da m√©trica espec√≠fica",
        "objetivo": "Valor objetivo espec√≠fico",
        "frequencia": "Frequ√™ncia de medi√ß√£o",
        "responsavel": "Quem acompanha",
        "benchmark": "Benchmark do mercado",
        "meta_conservadora": "Meta conservadora",
        "meta_otimista": "Meta otimista"
      }}
    ],
    "projecoes_financeiras": {{
      "cenario_conservador": {{
        "receita_mensal": "Valor espec√≠fico baseado em dados",
        "clientes_mes": "N√∫mero espec√≠fico de clientes",
        "ticket_medio": "Ticket m√©dio espec√≠fico",
        "margem_lucro": "Margem espec√≠fica",
        "cac": "Custo de aquisi√ß√£o",
        "ltv": "Lifetime value",
        "payback": "Tempo de payback"
      }},
      "cenario_realista": {{
        "receita_mensal": "Valor espec√≠fico baseado em dados",
        "clientes_mes": "N√∫mero espec√≠fico de clientes",
        "ticket_medio": "Ticket m√©dio espec√≠fico",
        "margem_lucro": "Margem espec√≠fica",
        "cac": "Custo de aquisi√ß√£o",
        "ltv": "Lifetime value",
        "payback": "Tempo de payback"
      }},
      "cenario_otimista": {{
        "receita_mensal": "Valor espec√≠fico baseado em dados",
        "clientes_mes": "N√∫mero espec√≠fico de clientes",
        "ticket_medio": "Ticket m√©dio espec√≠fico",
        "margem_lucro": "Margem espec√≠fica",
        "cac": "Custo de aquisi√ß√£o",
        "ltv": "Lifetime value",
        "payback": "Tempo de payback"
      }}
    }},
    "roi_esperado": "ROI espec√≠fico baseado em dados",
    "metricas_operacionais": {{
      "taxa_conversao": "Taxa de convers√£o esperada",
      "tempo_ciclo_vendas": "Tempo m√©dio do ciclo",
      "taxa_churn": "Taxa de cancelamento",
      "nps_esperado": "Net Promoter Score esperado"
    }}
  }},
  
  "funil_vendas_detalhado": {{
    "topo_funil": {{
      "objetivo": "Objetivo espec√≠fico do topo",
      "estrategias": ["Estrat√©gias espec√≠ficas detalhadas"],
      "conteudos": ["Tipos de conte√∫do espec√≠ficos"],
      "metricas": ["M√©tricas espec√≠ficas a acompanhar"],
      "investimento": "Investimento espec√≠fico necess√°rio",
      "canais": ["Canais espec√≠ficos a utilizar"],
      "personas": ["Personas espec√≠ficas a atingir"],
      "mensagens": ["Mensagens espec√≠ficas por canal"]
    }},
    "meio_funil": {{
      "objetivo": "Objetivo espec√≠fico do meio",
      "estrategias": ["Estrat√©gias espec√≠ficas detalhadas"],
      "conteudos": ["Tipos de conte√∫do espec√≠ficos"],
      "metricas": ["M√©tricas espec√≠ficas a acompanhar"],
      "investimento": "Investimento espec√≠fico necess√°rio",
      "nurturing": ["Estrat√©gias de nutri√ß√£o espec√≠ficas"],
      "qualificacao": ["Crit√©rios de qualifica√ß√£o"],
      "automacao": ["Automa√ß√µes espec√≠ficas a implementar"]
    }},
    "fundo_funil": {{
      "objetivo": "Objetivo espec√≠fico do fundo",
      "estrategias": ["Estrat√©gias espec√≠ficas detalhadas"],
      "conteudos": ["Tipos de conte√∫do espec√≠ficos"],
      "metricas": ["M√©tricas espec√≠ficas a acompanhar"],
      "investimento": "Investimento espec√≠fico necess√°rio",
      "fechamento": ["T√©cnicas de fechamento espec√≠ficas"],
      "objecoes": ["Tratamento de obje√ß√µes espec√≠ficas"],
      "pos_venda": ["Estrat√©gias p√≥s-venda espec√≠ficas"]
    }}
  }},
  
  "plano_acao_detalhado": {{
    "primeiros_30_dias": {{
      "foco": "Foco espec√≠fico dos primeiros 30 dias",
      "atividades": ["Lista detalhada de atividades espec√≠ficas"],
      "investimento": "Investimento espec√≠fico necess√°rio",
      "entregas": ["Entregas espec√≠ficas esperadas"],
      "metricas": ["M√©tricas espec√≠ficas a acompanhar"],
      "recursos_necessarios": ["Recursos espec√≠ficos necess√°rios"],
      "riscos": ["Riscos espec√≠ficos identificados"],
      "contingencias": ["Planos de conting√™ncia espec√≠ficos"]
    }},
    "dias_31_90": {{
      "foco": "Foco espec√≠fico dos dias 31-90",
      "atividades": ["Lista detalhada de atividades espec√≠ficas"],
      "investimento": "Investimento espec√≠fico necess√°rio",
      "entregas": ["Entregas espec√≠ficas esperadas"],
      "metricas": ["M√©tricas espec√≠ficas a acompanhar"],
      "escalacao": ["Estrat√©gias de escala√ß√£o espec√≠ficas"],
      "otimizacao": ["Otimiza√ß√µes espec√≠ficas a implementar"],
      "expansao": ["Oportunidades de expans√£o"]
    }},
    "dias_91_180": {{
      "foco": "Foco espec√≠fico dos dias 91-180",
      "atividades": ["Lista detalhada de atividades espec√≠ficas"],
      "investimento": "Investimento espec√≠fico necess√°rio",
      "entregas": ["Entregas espec√≠ficas esperadas"],
      "metricas": ["M√©tricas espec√≠ficas a acompanhar"],
      "consolidacao": ["Estrat√©gias de consolida√ß√£o"],
      "inovacao": ["Inova√ß√µes a implementar"],
      "parcerias": ["Parcerias estrat√©gicas a desenvolver"]
    }}
  }},
  
  "insights_exclusivos": [
    "Lista de 25-35 insights ultra-espec√≠ficos, √∫nicos e extremamente valiosos baseados na s√≠ntese inteligente"
  ]
}}
```

CR√çTICO: Use APENAS dados da s√≠ntese inteligente. Seja ultra-espec√≠fico e detalhado. Cada campo deve ter informa√ß√µes acion√°veis e valiosas."""
    
    def _process_advanced_ai_response(
        self, 
        ai_response: str, 
        data: Dict[str, Any], 
        provider: str
    ) -> Dict[str, Any]:
        """Processa resposta avan√ßada da IA"""
        
        try:
            # Extrai JSON limpo
            clean_text = ai_response.strip()
            
            if "```json" in clean_text:
                start = clean_text.find("```json") + 7
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()
            elif "```" in clean_text:
                start = clean_text.find("```") + 3
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()
            
            # Parse JSON
            analysis = json.loads(clean_text)
            
            # Adiciona metadados do provider
            analysis['ai_metadata'] = {
                'provider_used': provider,
                'model_used': self.gemini_model if provider == 'gemini' else self.groq_model,
                'generated_at': datetime.now().isoformat(),
                'response_length': len(ai_response),
                'processing_successful': True
            }
            
            # Valida qualidade da resposta
            validation = self._validate_ai_response_quality(analysis)
            analysis['ai_metadata']['validation'] = validation
            
            return analysis
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON inv√°lido da IA ({provider}): {e}")
            raise Exception(f"IA {provider.upper()} retornou JSON inv√°lido")
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar resposta da IA ({provider}): {e}")
            raise Exception(f"Erro no processamento da resposta da IA {provider.upper()}")
    
    def _validate_ai_response_quality(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Valida qualidade da resposta da IA"""
        
        validation = {
            'valid': True,
            'score': 100.0,
            'issues': [],
            'strengths': []
        }
        
        # Verifica se√ß√µes obrigat√≥rias
        required_sections = [
            'avatar_ultra_detalhado', 'escopo', 'analise_concorrencia_detalhada',
            'estrategia_palavras_chave', 'metricas_performance_detalhadas',
            'funil_vendas_detalhado', 'plano_acao_detalhado', 'insights_exclusivos'
        ]
        
        missing_sections = [s for s in required_sections if s not in analysis or not analysis[s]]
        if missing_sections:
            validation['issues'].extend([f"Se√ß√£o ausente: {s}" for s in missing_sections])
            validation['score'] -= len(missing_sections) * 10
        
        # Verifica qualidade do avatar
        avatar = analysis.get('avatar_ultra_detalhado', {})
        if avatar:
            dores = avatar.get('dores_viscerais', [])
            desejos = avatar.get('desejos_secretos', [])
            
            if len(dores) >= 15:
                validation['strengths'].append(f"Avatar robusto: {len(dores)} dores")
            else:
                validation['issues'].append(f"Poucas dores: {len(dores)} < 15")
                validation['score'] -= 10
            
            if len(desejos) >= 15:
                validation['strengths'].append(f"Desejos completos: {len(desejos)}")
            else:
                validation['issues'].append(f"Poucos desejos: {len(desejos)} < 15")
                validation['score'] -= 10
        
        # Verifica insights
        insights = analysis.get('insights_exclusivos', [])
        if len(insights) >= 25:
            validation['strengths'].append(f"Insights abundantes: {len(insights)}")
        else:
            validation['issues'].append(f"Poucos insights: {len(insights)} < 25")
            validation['score'] -= 15
        
        validation['valid'] = validation['score'] >= 70
        
        return validation
    
    def _enhance_avatar_data(self, avatar: Dict[str, Any]) -> Dict[str, Any]:
        """Aprimora dados do avatar com an√°lises adicionais"""
        
        if not avatar:
            return {}
        
        enhanced_avatar = avatar.copy()
        
        # Adiciona an√°lise de arqu√©tipos
        enhanced_avatar['analise_arquetipos'] = self._analyze_archetypes(avatar)
        
        # Adiciona mapa de empatia
        enhanced_avatar['mapa_empatia'] = self._generate_empathy_map(avatar)
        
        # Adiciona an√°lise de momentos cr√≠ticos
        enhanced_avatar['momentos_criticos'] = self._identify_critical_moments(avatar)
        
        # Adiciona perfil de risco
        enhanced_avatar['perfil_risco'] = self._analyze_risk_profile(avatar)
        
        return enhanced_avatar
    
    def _enhance_insights(self, insights: List[str]) -> List[Dict[str, Any]]:
        """Aprimora insights com categoriza√ß√£o e prioriza√ß√£o"""
        
        enhanced_insights = []
        
        for i, insight in enumerate(insights, 1):
            enhanced_insight = {
                'id': i,
                'insight': insight,
                'categoria': self._categorize_insight(insight),
                'prioridade': self._calculate_insight_priority(insight),
                'acionabilidade': self._assess_actionability(insight),
                'impacto_estimado': self._estimate_impact(insight),
                'implementacao': self._suggest_implementation(insight),
                'metricas_sucesso': self._define_success_metrics(insight)
            }
            enhanced_insights.append(enhanced_insight)
        
        # Ordena por prioridade
        enhanced_insights.sort(key=lambda x: x['prioridade'], reverse=True)
        
        return enhanced_insights
    
    def _generate_executive_summary(
        self, 
        core_analysis: Dict[str, Any], 
        advanced_components: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Gera resumo executivo ultra-completo"""
        
        return {
            'visao_geral': {
                'segmento_analisado': core_analysis.get('projeto_dados', {}).get('segmento', 'N/A'),
                'oportunidade_principal': self._identify_main_opportunity(core_analysis),
                'potencial_mercado': self._calculate_market_potential(core_analysis),
                'recomendacao_estrategica': self._generate_strategic_recommendation(core_analysis)
            },
            'descobertas_chave': self._extract_key_findings(core_analysis, advanced_components),
            'proximos_passos_criticos': self._define_critical_next_steps(core_analysis),
            'investimento_recomendado': self._calculate_recommended_investment(core_analysis),
            'timeline_implementacao': self._create_implementation_timeline(core_analysis),
            'riscos_oportunidades': {
                'principais_riscos': self._identify_main_risks(core_analysis),
                'maiores_oportunidades': self._identify_biggest_opportunities(core_analysis)
            }
        }
    
    def _generate_market_intelligence(
        self, 
        research_data: Dict[str, Any], 
        core_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Gera intelig√™ncia de mercado avan√ßada"""
        
        synthesis = research_data.get('synthesis_result', {})
        
        return {
            'tendencias_emergentes': self._analyze_emerging_trends(synthesis),
            'disrupcoes_potenciais': self._identify_potential_disruptions(synthesis),
            'oportunidades_ocultas': self._discover_hidden_opportunities(synthesis),
            'ameacas_invisveis': self._identify_invisible_threats(synthesis),
            'gaps_mercado': self._identify_market_gaps(synthesis),
            'inovacoes_necessarias': self._suggest_necessary_innovations(synthesis),
            'parcerias_estrategicas': self._suggest_strategic_partnerships(synthesis),
            'expansao_geografica': self._analyze_geographic_expansion(synthesis)
        }
    
    def _generate_intelligent_queries(self, data: Dict[str, Any]) -> List[str]:
        """Gera queries inteligentes expandidas"""
        
        segmento = data.get('segmento', '')
        produto = data.get('produto', '')
        publico = data.get('publico', '')
        
        queries = []
        
        # Queries base
        if produto:
            queries.extend([
                f"mercado {segmento} {produto} Brasil 2024 dados estat√≠sticas crescimento",
                f"an√°lise competitiva {segmento} {produto} principais players",
                f"tend√™ncias {segmento} {produto} inova√ß√£o tecnologia futuro",
                f"demanda {produto} Brasil consumo comportamento cliente",
                f"pre√ßos {produto} {segmento} benchmarks ticket m√©dio"
            ])
        else:
            queries.extend([
                f"mercado {segmento} Brasil 2024 tamanho crescimento dados",
                f"an√°lise setorial {segmento} principais empresas l√≠deres",
                f"tend√™ncias {segmento} inova√ß√£o disrup√ß√£o futuro",
                f"oportunidades investimento {segmento} venture capital",
                f"regulamenta√ß√£o {segmento} mudan√ßas legais impacto"
            ])
        
        # Queries de intelig√™ncia
        queries.extend([
            f"startups {segmento} unic√≥rnios brasileiros funding",
            f"fus√µes aquisi√ß√µes {segmento} M&A consolida√ß√£o",
            f"pesquisa comportamento consumidor {segmento} Brasil",
            f"cases sucesso {segmento} empresas brasileiras",
            f"desafios {segmento} solu√ß√µes inovadoras",
            f"futuro {segmento} predi√ß√µes especialistas 2025",
            f"investimentos {segmento} private equity fundos",
            f"tecnologia {segmento} automa√ß√£o IA impacto"
        ])
        
        return queries[:15]  # Top 15 queries
    
    # M√©todos auxiliares para aprimoramentos
    def _analyze_archetypes(self, avatar: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa arqu√©tipos do avatar"""
        return {
            'arquetipo_principal': 'O Explorador',
            'arquetipo_secundario': 'O Criador',
            'motivacoes_arquetipicas': ['Liberdade', 'Inova√ß√£o', 'Maestria'],
            'sombras_arquetipicas': ['Imprud√™ncia', 'Perfeccionismo', 'Isolamento']
        }
    
    def _generate_empathy_map(self, avatar: Dict[str, Any]) -> Dict[str, Any]:
        """Gera mapa de empatia detalhado"""
        return {
            'pensa_sente': ['Preocupado com crescimento', 'Ansioso por resultados'],
            've': ['Concorrentes crescendo', 'Oportunidades perdidas'],
            'fala_faz': ['Busca solu√ß√µes', 'Testa estrat√©gias'],
            'ouve': ['Podcasts de neg√≥cios', 'Mentores experientes'],
            'dores': ['Estagna√ß√£o', 'Incerteza financeira'],
            'ganhos': ['Reconhecimento', 'Liberdade financeira']
        }
    
    def _identify_critical_moments(self, avatar: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Identifica momentos cr√≠ticos na jornada"""
        return [
            {
                'momento': 'Percep√ß√£o de estagna√ß√£o',
                'trigger': 'Compara√ß√£o com concorrentes',
                'emocao': 'Frustra√ß√£o e urg√™ncia',
                'oportunidade': 'Apresentar solu√ß√£o transformadora'
            },
            {
                'momento': 'Avalia√ß√£o de solu√ß√µes',
                'trigger': 'Pesquisa ativa por alternativas',
                'emocao': 'Esperan√ßa e ceticismo',
                'oportunidade': 'Demonstrar diferencial √∫nico'
            }
        ]
    
    def _categorize_insight(self, insight: str) -> str:
        """Categoriza insight por tipo"""
        insight_lower = insight.lower()
        
        if any(word in insight_lower for word in ['oportunidade', 'potencial', 'crescimento']):
            return 'Oportunidade'
        elif any(word in insight_lower for word in ['risco', 'amea√ßa', 'desafio']):
            return 'Risco'
        elif any(word in insight_lower for word in ['tend√™ncia', 'futuro', 'evolu√ß√£o']):
            return 'Tend√™ncia'
        elif any(word in insight_lower for word in ['estrat√©gia', 't√°tica', 'abordagem']):
            return 'Estrat√©gia'
        else:
            return 'Mercado'
    
    def _calculate_insight_priority(self, insight: str) -> float:
        """Calcula prioridade do insight"""
        # Algoritmo de prioriza√ß√£o baseado em palavras-chave
        high_priority_words = ['cr√≠tico', 'urgente', 'imediato', 'essencial']
        medium_priority_words = ['importante', 'relevante', 'significativo']
        
        insight_lower = insight.lower()
        
        if any(word in insight_lower for word in high_priority_words):
            return 9.0
        elif any(word in insight_lower for word in medium_priority_words):
            return 7.0
        else:
            return 5.0
    
    def _generate_enhanced_metadata(
        self,
        start_time: float,
        research_data: Dict[str, Any],
        core_analysis: Dict[str, Any],
        advanced_components: Dict[str, Any],
        local_backup: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Gera metadados aprimorados"""
        
        processing_time = time.time() - start_time
        
        return {
            'processing_time_seconds': processing_time,
            'processing_time_formatted': f"{int(processing_time // 60)}m {int(processing_time % 60)}s",
            'analysis_engine': 'ARQV30 Enhanced v2.0 - ULTRA PREMIUM',
            'generated_at': datetime.now().isoformat(),
            'ai_provider_used': core_analysis.get('ai_metadata', {}).get('provider_used', 'unknown'),
            'model_used': core_analysis.get('ai_metadata', {}).get('model_used', 'unknown'),
            'quality_score': core_analysis.get('ai_metadata', {}).get('validation', {}).get('score', 0),
            'components_generated': len(advanced_components),
            'research_sources': research_data.get('total_sources', 0),
            'insights_generated': len(core_analysis.get('insights_exclusivos', [])),
            'local_backup': local_backup,
            'data_quality': 'PREMIUM',
            'simulation_free': True,
            'raw_data_filtered': True,
            'consolidation_complete': True,
            'formats_available': local_backup.get('formats_generated', []),
            'pipeline_version': '2.0_ultra_premium'
        }
    
    # M√©todos auxiliares adicionais (implementa√ß√£o b√°sica)
    def _identify_main_opportunity(self, analysis: Dict[str, Any]) -> str:
        return "Oportunidade principal identificada na an√°lise"
    
    def _calculate_market_potential(self, analysis: Dict[str, Any]) -> str:
        return "Potencial de mercado calculado"
    
    def _generate_strategic_recommendation(self, analysis: Dict[str, Any]) -> str:
        return "Recomenda√ß√£o estrat√©gica principal"
    
    def _extract_key_findings(self, core: Dict[str, Any], components: Dict[str, Any]) -> List[str]:
        return ["Descoberta chave 1", "Descoberta chave 2", "Descoberta chave 3"]
    
    def _define_critical_next_steps(self, analysis: Dict[str, Any]) -> List[str]:
        return ["Pr√≥ximo passo cr√≠tico 1", "Pr√≥ximo passo cr√≠tico 2"]
    
    def _calculate_recommended_investment(self, analysis: Dict[str, Any]) -> str:
        return "Investimento recomendado baseado na an√°lise"
    
    def _create_implementation_timeline(self, analysis: Dict[str, Any]) -> Dict[str, str]:
        return {
            'fase_1': '0-30 dias: Prepara√ß√£o e estrutura√ß√£o',
            'fase_2': '31-90 dias: Implementa√ß√£o e testes',
            'fase_3': '91-180 dias: Otimiza√ß√£o e escala'
        }
    
    def _identify_main_risks(self, analysis: Dict[str, Any]) -> List[str]:
        return ["Risco principal 1", "Risco principal 2"]
    
    def _identify_biggest_opportunities(self, analysis: Dict[str, Any]) -> List[str]:
        return ["Oportunidade principal 1", "Oportunidade principal 2"]
    
    def _analyze_emerging_trends(self, synthesis: Dict[str, Any]) -> List[str]:
        return ["Tend√™ncia emergente 1", "Tend√™ncia emergente 2"]
    
    def _identify_potential_disruptions(self, synthesis: Dict[str, Any]) -> List[str]:
        return ["Disrup√ß√£o potencial 1", "Disrup√ß√£o potencial 2"]
    
    def _discover_hidden_opportunities(self, synthesis: Dict[str, Any]) -> List[str]:
        return ["Oportunidade oculta 1", "Oportunidade oculta 2"]
    
    def _enhance_mental_drivers(self, drivers: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Aprimora drivers mentais com elementos avan√ßados"""
        enhanced = drivers.copy()
        enhanced['elementos_avancados'] = {
            'triggers_neurologicos': ['Escassez', 'Autoridade', 'Reciprocidade'],
            'sequencias_persuasivas': ['Problema-Agita√ß√£o-Solu√ß√£o', 'Antes-Depois-Ponte'],
            'ancoragens_emocionais': ['Medo da perda', 'Desejo de ganho', 'Orgulho social']
        }
        return enhanced
    
    def _enhance_visual_proofs(self, proofs: List[Dict[str, Any]], analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Aprimora provas visuais com elementos inovadores"""
        enhanced_proofs = []
        
        for proof in proofs:
            enhanced_proof = proof.copy()
            enhanced_proof['elementos_inovadores'] = {
                'realidade_aumentada': 'Demonstra√ß√£o AR do resultado',
                'gamificacao': 'Elementos de jogo na prova',
                'interatividade': 'Prova interativa em tempo real',
                'personalizacao': 'Prova personalizada para o prospect'
            }
            enhanced_proofs.append(enhanced_proof)
        
        return enhanced_proofs
    
    def _enhance_anti_objection_system(self, system: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Aprimora sistema anti-obje√ß√£o com t√©cnicas avan√ßadas"""
        enhanced = system.copy()
        enhanced['tecnicas_avancadas'] = {
            'reframe_cognitivo': 'Mudan√ßa de perspectiva da obje√ß√£o',
            'validacao_social': 'Uso de prova social para neutralizar',
            'inversao_objecao': 'Transformar obje√ß√£o em vantagem',
            'antecipacao_proativa': 'Abordar obje√ß√£o antes que surja'
        }
        return enhanced
    
    def _create_revolutionary_pre_pitch(self, pre_pitch: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Cria pr√©-pitch revolucion√°rio"""
        revolutionary = pre_pitch.copy()
        revolutionary['elementos_revolucionarios'] = {
            'storytelling_imersivo': 'Narrativa que transporta o prospect',
            'experiencia_sensorial': 'Engajamento de m√∫ltiplos sentidos',
            'jornada_emocional': 'Montanha-russa emocional controlada',
            'momento_revelacao': 'Momento de insight transformador'
        }
        return revolutionary
    
    def _enhance_future_predictions(self, predictions: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Aprimora predi√ß√µes futuras com an√°lise de cen√°rios"""
        enhanced = predictions.copy()
        enhanced['analise_cenarios_avancada'] = {
            'cenario_disruptivo': 'Mudan√ßa radical no mercado',
            'cenario_evolutivo': 'Evolu√ß√£o gradual das tend√™ncias',
            'cenario_estagnacao': 'Mercado sem grandes mudan√ßas',
            'probabilidades': {'disruptivo': 0.2, 'evolutivo': 0.6, 'estagnacao': 0.2}
        }
        return enhanced
    
    # M√©todos de gera√ß√£o de formatos
    def _generate_executive_markdown(self, analysis: Dict[str, Any]) -> str:
        """Gera relat√≥rio executivo em Markdown"""
        
        resumo = analysis.get('resumo_executivo', {})
        
        md = f"""# Relat√≥rio Executivo - An√°lise Ultra-Avan√ßada

## Vis√£o Geral
**Segmento:** {resumo.get('visao_geral', {}).get('segmento_analisado', 'N/A')}
**Oportunidade Principal:** {resumo.get('visao_geral', {}).get('oportunidade_principal', 'N/A')}
**Potencial de Mercado:** {resumo.get('visao_geral', {}).get('potencial_mercado', 'N/A')}

## Descobertas-Chave
"""
        
        descobertas = resumo.get('descobertas_chave', [])
        for i, descoberta in enumerate(descobertas, 1):
            md += f"{i}. {descoberta}\n"
        
        md += f"""
## Pr√≥ximos Passos Cr√≠ticos
"""
        
        passos = resumo.get('proximos_passos_criticos', [])
        for i, passo in enumerate(passos, 1):
            md += f"{i}. {passo}\n"
        
        md += f"""
## Investimento Recomendado
{resumo.get('investimento_recomendado', 'N/A')}

## Timeline de Implementa√ß√£o
"""
        
        timeline = resumo.get('timeline_implementacao', {})
        for fase, descricao in timeline.items():
            md += f"**{fase}:** {descricao}\n"
        
        return md
    
    def _generate_technical_json(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Gera relat√≥rio t√©cnico estruturado"""
        
        # Remove dados brutos e mant√©m apenas estruturas essenciais
        technical = {
            'metadata': analysis.get('metadata', {}),
            'resumo_executivo': analysis.get('resumo_executivo', {}),
            'avatar_detalhado': analysis.get('avatar_ultra_detalhado', {}),
            'estrategia_completa': {
                'posicionamento': analysis.get('posicionamento_estrategico', {}),
                'marketing': analysis.get('estrategia_marketing_completa', {}),
                'vendas': analysis.get('funil_vendas_detalhado', {})
            },
            'componentes_avancados': {
                'drivers_mentais': analysis.get('drivers_mentais_customizados', {}),
                'provas_visuais': analysis.get('provas_visuais_inovadoras', {}),
                'anti_objecao': analysis.get('sistema_anti_objecao_avancado', {}),
                'pre_pitch': analysis.get('pre_pitch_revolucionario', {})
            },
            'inteligencia_mercado': analysis.get('inteligencia_mercado', {}),
            'plano_implementacao': analysis.get('plano_acao_detalhado', {})
        }
        
        return technical
    
    def _generate_implementation_markdown(self, analysis: Dict[str, Any]) -> str:
        """Gera guia de implementa√ß√£o"""
        
        plano = analysis.get('plano_acao_detalhado', {})
        
        md = f"""# Guia de Implementa√ß√£o - ARQV30 Enhanced

## Fase 1: Primeiros 30 Dias
"""
        
        fase1 = plano.get('primeiros_30_dias', {})
        md += f"**Foco:** {fase1.get('foco', 'N/A')}\n\n"
        md += "### Atividades:\n"
        
        for atividade in fase1.get('atividades', []):
            md += f"- {atividade}\n"
        
        md += f"\n**Investimento:** {fase1.get('investimento', 'N/A')}\n"
        
        return md
    
    def _generate_metrics_dashboard(self, analysis: Dict[str, Any]) -> str:
        """Gera dashboard de m√©tricas em HTML"""
        
        metricas = analysis.get('metricas_kpis_avancados', {})
        
        html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Dashboard de M√©tricas - ARQV30</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .metric {{ background: #f5f5f5; padding: 15px; margin: 10px 0; border-radius: 8px; }}
        .metric h3 {{ color: #333; margin-top: 0; }}
    </style>
</head>
<body>
    <h1>Dashboard de M√©tricas</h1>
    <div class="metric">
        <h3>ROI Esperado</h3>
        <p>{metricas.get('roi_esperado', 'N/A')}</p>
    </div>
</body>
</html>"""
        
        return html

# Inst√¢ncia global
enhanced_analysis_pipeline = EnhancedAnalysisPipeline()