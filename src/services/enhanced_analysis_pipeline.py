#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Enhanced Analysis Pipeline
Pipeline de anÃ¡lise aprimorado com Gemini 2.5 Pro e fallback Groq
"""

import time
import logging
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Callable
from services.ai_manager import ai_manager
from services.production_search_manager import production_search_manager
from services.robust_content_extractor import robust_content_extractor
from services.content_synthesis_engine import content_synthesis_engine
from services.mental_drivers_architect import mental_drivers_architect
from services.visual_proofs_generator import visual_proofs_generator
from services.anti_objection_system import anti_objection_system
from services.enhanced_pre_pitch_architect import enhanced_pre_pitch_architect
from services.future_prediction_engine import future_prediction_engine
from services.local_file_manager import local_file_manager
from services.auto_save_manager import salvar_etapa, salvar_erro

logger = logging.getLogger(__name__)

class EnhancedAnalysisPipeline:
    """Pipeline de anÃ¡lise aprimorado com IA avanÃ§ada e consolidaÃ§Ã£o robusta"""
    
    def __init__(self):
        """Inicializa pipeline aprimorado"""
        self.gemini_model = "gemini-2.0-flash-exp"  # Modelo mais avanÃ§ado
        self.groq_model = "llama-3.3-70b-versatile"  # Modelo Groq atualizado
        
        self.quality_thresholds = {
            'min_sources': 5,
            'min_content_length': 8000,
            'min_quality_score': 75.0,
            'min_insights': 20,
            'min_avatar_depth': 10
        }
        
        self.consolidation_rules = {
            'remove_raw_data': True,
            'enhance_insights': True,
            'validate_completeness': True,
            'ensure_local_backup': True,
            'generate_executive_summary': True
        }
        
        logger.info("Enhanced Analysis Pipeline inicializado com Gemini 2.5 Pro")
    
    def execute_complete_analysis(
        self, 
        data: Dict[str, Any],
        session_id: Optional[str] = None,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """Executa anÃ¡lise completa aprimorada"""
        
        start_time = time.time()
        logger.info(f"ðŸš€ Iniciando anÃ¡lise aprimorada para {data.get('segmento')}")
        
        try:
            # Fase 1: Pesquisa e SÃ­ntese Inteligente
            if progress_callback:
                progress_callback(1, "ðŸ” Executando pesquisa inteligente massiva...")
            
            research_data = self._execute_intelligent_research(data, progress_callback)
            
            # Fase 2: AnÃ¡lise com Gemini 2.5 Pro (fallback Groq)
            if progress_callback:
                progress_callback(3, "ðŸ§  Analisando com Gemini 2.5 Pro...")
            
            core_analysis = self._execute_advanced_ai_analysis(data, research_data)
            
            # Fase 3: Componentes AvanÃ§ados Robustos
            if progress_callback:
                progress_callback(5, "âš¡ Gerando componentes ultra-robustos...")
            
            advanced_components = self._generate_robust_components(core_analysis, data, progress_callback)
            
            # Fase 4: ConsolidaÃ§Ã£o e Aprimoramento
            if progress_callback:
                progress_callback(10, "ðŸ“Š Consolidando relatÃ³rio final...")
            
            final_analysis = self._consolidate_enhanced_analysis(
                data, research_data, core_analysis, advanced_components
            )
            
            # Fase 5: Salvamento Local Garantido
            if progress_callback:
                progress_callback(12, "ðŸ’¾ Salvando relatÃ³rios localmente...")
            
            local_backup = self._ensure_local_backup(final_analysis, session_id)
            
            # Adiciona metadados finais
            final_analysis['metadata'] = self._generate_enhanced_metadata(
                start_time, research_data, core_analysis, advanced_components, local_backup
            )
            
            if progress_callback:
                progress_callback(13, "âœ… AnÃ¡lise aprimorada concluÃ­da!")
            
            logger.info(f"âœ… AnÃ¡lise aprimorada concluÃ­da em {time.time() - start_time:.2f}s")
            return final_analysis
            
        except Exception as e:
            logger.error(f"âŒ Erro na anÃ¡lise aprimorada: {str(e)}")
            salvar_erro("pipeline_aprimorado", e, contexto=data)
            raise Exception(f"ANÃLISE APRIMORADA FALHOU: {str(e)}")
    
    def _execute_intelligent_research(
        self, 
        data: Dict[str, Any], 
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """Executa pesquisa inteligente com sÃ­ntese avanÃ§ada"""
        
        logger.info("ðŸ” Iniciando pesquisa inteligente massiva")
        
        # Gera queries inteligentes expandidas
        queries = self._generate_intelligent_queries(data)
        
        # Executa busca multi-fonte
        all_results = []
        extracted_content = []
        
        for i, query in enumerate(queries):
            if progress_callback:
                progress_callback(1, f"ðŸ” Pesquisando: {query[:50]}...", f"Query {i+1}/{len(queries)}")
            
            try:
                # Busca com mÃºltiplos provedores
                search_results = production_search_manager.search_with_fallback(query, max_results=12)
                all_results.extend(search_results)
                
                # Extrai conteÃºdo com validaÃ§Ã£o
                for result in search_results[:8]:
                    content = robust_content_extractor.extract_content(result['url'])
                    if content and len(content) > 500:
                        extracted_content.append({
                            'url': result['url'],
                            'title': result.get('title', ''),
                            'content': content,
                            'query': query,
                            'source': result.get('source', 'unknown')
                        })
                
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                logger.warning(f"âš ï¸ Erro na query '{query}': {e}")
                continue
        
        # SÃ­ntese inteligente do conteÃºdo
        if progress_callback:
            progress_callback(2, "ðŸ§  Sintetizando conteÃºdo extraÃ­do...")
        
        synthesis_result = content_synthesis_engine.synthesize_research_content(
            extracted_content, data
        )
        
        # Salva dados de pesquisa (sem conteÃºdo bruto)
        research_summary = {
            'queries_executed': queries,
            'total_sources': len(extracted_content),
            'synthesis_result': synthesis_result,
            'statistics': {
                'total_queries': len(queries),
                'total_results': len(all_results),
                'successful_extractions': len(extracted_content),
                'total_content_length': sum(len(c['content']) for c in extracted_content),
                'unique_domains': len(set(c['url'].split('/')[2] for c in extracted_content)),
                'avg_content_length': sum(len(c['content']) for c in extracted_content) / len(extracted_content) if extracted_content else 0
            }
        }
        
        salvar_etapa("pesquisa_inteligente", research_summary, categoria="pesquisa_web")
        
        logger.info(f"âœ… Pesquisa inteligente: {len(extracted_content)} fontes, sÃ­ntese com {len(synthesis_result.get('categorized_insights', {}).get('all_insights', []))} insights")
        
        return research_summary
    
    def _execute_advanced_ai_analysis(
        self, 
        data: Dict[str, Any], 
        research_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Executa anÃ¡lise com Gemini 2.5 Pro (fallback Groq)"""
        
        logger.info("ðŸ§  Executando anÃ¡lise com IA avanÃ§ada")
        
        # Prepara contexto de sÃ­ntese
        synthesis_context = self._prepare_synthesis_context(research_data)
        
        # ConstrÃ³i prompt ultra-avanÃ§ado
        prompt = self._build_advanced_analysis_prompt(data, synthesis_context)
        
        # Tenta Gemini 2.5 Pro primeiro
        try:
            logger.info("ðŸš€ Tentando anÃ¡lise com Gemini 2.5 Pro...")
            
            ai_response = ai_manager.generate_analysis(
                prompt, 
                max_tokens=8192,
                provider='gemini'  # ForÃ§a uso do Gemini
            )
            
            if ai_response:
                analysis = self._process_advanced_ai_response(ai_response, data, 'gemini')
                logger.info("âœ… AnÃ¡lise concluÃ­da com Gemini 2.5 Pro")
                return analysis
            else:
                raise Exception("Gemini nÃ£o retornou resposta vÃ¡lida")
                
        except Exception as e:
            logger.warning(f"âš ï¸ Gemini falhou: {e}")
            
            # Fallback para Groq
            try:
                logger.info("ðŸ”„ Fallback para Groq...")
                
                ai_response = ai_manager.generate_analysis(
                    prompt,
                    max_tokens=8192,
                    provider='groq'  # ForÃ§a uso do Groq
                )
                
                if ai_response:
                    analysis = self._process_advanced_ai_response(ai_response, data, 'groq')
                    logger.info("âœ… AnÃ¡lise concluÃ­da com Groq (fallback)")
                    return analysis
                else:
                    raise Exception("Groq tambÃ©m falhou")
                    
            except Exception as groq_error:
                logger.error(f"âŒ Groq tambÃ©m falhou: {groq_error}")
                raise Exception(f"AMBAS IAs FALHARAM: Gemini ({e}) | Groq ({groq_error})")
    
    def _generate_robust_components(
        self, 
        core_analysis: Dict[str, Any], 
        data: Dict[str, Any],
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """Gera componentes avanÃ§ados ultra-robustos"""
        
        components = {}
        
        # Drivers Mentais Ultra-Robustos
        if progress_callback:
            progress_callback(6, "ðŸ§  Gerando drivers mentais ultra-robustos...")
        
        try:
            avatar_data = core_analysis.get('avatar_ultra_detalhado', {})
            drivers_system = mental_drivers_architect.generate_complete_drivers_system(
                avatar_data, data
            )
            
            if drivers_system and not drivers_system.get('fallback_mode'):
                # Aprimora drivers com anÃ¡lise adicional
                enhanced_drivers = self._enhance_mental_drivers(drivers_system, core_analysis)
                components['drivers_mentais_customizados'] = enhanced_drivers
                salvar_etapa("drivers_ultra_robustos", enhanced_drivers, categoria="drivers_mentais")
                logger.info("âœ… Drivers mentais ultra-robustos gerados")
            
        except Exception as e:
            logger.error(f"âŒ Erro nos drivers mentais: {e}")
            salvar_erro("drivers_mentais", e)
        
        # Provas Visuais Inovadoras
        if progress_callback:
            progress_callback(7, "ðŸŽ­ Criando provas visuais inovadoras...")
        
        try:
            concepts = self._extract_proof_concepts(core_analysis, data)
            visual_proofs = visual_proofs_generator.generate_complete_proofs_system(
                concepts, avatar_data, data
            )
            
            if visual_proofs:
                # Aprimora provas com elementos inovadores
                enhanced_proofs = self._enhance_visual_proofs(visual_proofs, core_analysis)
                components['provas_visuais_inovadoras'] = enhanced_proofs
                salvar_etapa("provas_inovadoras", enhanced_proofs, categoria="provas_visuais")
                logger.info("âœ… Provas visuais inovadoras criadas")
            
        except Exception as e:
            logger.error(f"âŒ Erro nas provas visuais: {e}")
            salvar_erro("provas_visuais", e)
        
        # Sistema Anti-ObjeÃ§Ã£o AvanÃ§ado
        if progress_callback:
            progress_callback(8, "ðŸ›¡ï¸ Construindo sistema anti-objeÃ§Ã£o avanÃ§ado...")
        
        try:
            objections = avatar_data.get('objecoes_reais', [])
            if not objections:
                objections = self._generate_intelligent_objections(core_analysis, data)
            
            anti_objection = anti_objection_system.generate_complete_anti_objection_system(
                objections, avatar_data, data
            )
            
            if anti_objection and not anti_objection.get('fallback_mode'):
                # Aprimora sistema com tÃ©cnicas avanÃ§adas
                enhanced_anti_objection = self._enhance_anti_objection_system(anti_objection, core_analysis)
                components['sistema_anti_objecao_avancado'] = enhanced_anti_objection
                salvar_etapa("anti_objecao_avancado", enhanced_anti_objection, categoria="anti_objecao")
                logger.info("âœ… Sistema anti-objeÃ§Ã£o avanÃ§ado construÃ­do")
            
        except Exception as e:
            logger.error(f"âŒ Erro no sistema anti-objeÃ§Ã£o: {e}")
            salvar_erro("anti_objecao", e)
        
        # PrÃ©-Pitch RevolucionÃ¡rio
        if progress_callback:
            progress_callback(9, "ðŸŽ¯ Arquitetando prÃ©-pitch revolucionÃ¡rio...")
        
        try:
            drivers_data = components.get('drivers_mentais_customizados', {})
            pre_pitch = enhanced_pre_pitch_architect.generate_enhanced_pre_pitch_system(
                drivers_data, avatar_data, data
            )
            
            if pre_pitch and pre_pitch.get('validacao_status') == 'ENHANCED_VALID':
                # Adiciona elementos revolucionÃ¡rios
                revolutionary_pre_pitch = self._create_revolutionary_pre_pitch(pre_pitch, core_analysis)
                components['pre_pitch_revolucionario'] = revolutionary_pre_pitch
                salvar_etapa("pre_pitch_revolucionario", revolutionary_pre_pitch, categoria="pre_pitch")
                logger.info("âœ… PrÃ©-pitch revolucionÃ¡rio arquitetado")
            
        except Exception as e:
            logger.error(f"âŒ Erro no prÃ©-pitch: {e}")
            salvar_erro("pre_pitch", e)
        
        # PrediÃ§Ãµes Futuras AvanÃ§adas
        try:
            future_predictions = future_prediction_engine.predict_market_future(
                data.get('segmento', 'negÃ³cios'), data, horizon_months=48
            )
            
            if future_predictions:
                # Aprimora prediÃ§Ãµes com anÃ¡lise de cenÃ¡rios
                enhanced_predictions = self._enhance_future_predictions(future_predictions, core_analysis)
                components['predicoes_futuro_avancadas'] = enhanced_predictions
                salvar_etapa("predicoes_avancadas", enhanced_predictions, categoria="predicoes_futuro")
                logger.info("âœ… PrediÃ§Ãµes futuras avanÃ§adas geradas")
            
        except Exception as e:
            logger.error(f"âŒ Erro nas prediÃ§Ãµes: {e}")
            salvar_erro("predicoes_futuro", e)
        
        return components
    
    def _consolidate_enhanced_analysis(
        self,
        data: Dict[str, Any],
        research_data: Dict[str, Any],
        core_analysis: Dict[str, Any],
        advanced_components: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Consolida anÃ¡lise aprimorada sem dados brutos"""
        
        logger.info("ðŸ“Š Consolidando anÃ¡lise aprimorada")
        
        # Estrutura base consolidada
        consolidated = {
            'projeto_dados': data,
            'resumo_executivo': self._generate_executive_summary(core_analysis, advanced_components),
            'avatar_ultra_detalhado': self._enhance_avatar_data(core_analysis.get('avatar_ultra_detalhado', {})),
            'posicionamento_estrategico': self._enhance_positioning(core_analysis.get('escopo', {})),
            'analise_concorrencia_avancada': self._enhance_competition_analysis(core_analysis.get('analise_concorrencia_detalhada', [])),
            'estrategia_marketing_completa': self._enhance_marketing_strategy(core_analysis.get('estrategia_palavras_chave', {})),
            'metricas_kpis_avancados': self._enhance_metrics_analysis(core_analysis.get('metricas_performance_detalhadas', {})),
            'funil_vendas_detalhado': self._enhance_sales_funnel(core_analysis.get('funil_vendas_detalhado', {})),
            'plano_acao_detalhado': self._enhance_action_plan(core_analysis.get('plano_acao_detalhado', {})),
            'insights_exclusivos': self._enhance_insights(core_analysis.get('insights_exclusivos', [])),
            'inteligencia_mercado': self._generate_market_intelligence(research_data, core_analysis),
            'analise_oportunidades': self._generate_opportunity_analysis(core_analysis, advanced_components),
            'roadmap_implementacao': self._generate_implementation_roadmap(core_analysis, data)
        }
        
        # Adiciona componentes avanÃ§ados
        consolidated.update(advanced_components)
        
        # Remove dados brutos e adiciona apenas estatÃ­sticas
        consolidated['pesquisa_web_massiva'] = {
            'estatisticas': research_data.get('statistics', {}),
            'qualidade_dados': 'PREMIUM - Dados reais validados',
            'fontes_consultadas': research_data.get('total_sources', 0),
            'insights_sintetizados': len(research_data.get('synthesis_result', {}).get('categorized_insights', {}).get('all_insights', []))
        }
        
        return consolidated
    
    def _ensure_local_backup(
        self, 
        analysis: Dict[str, Any], 
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Garante backup local completo"""
        
        logger.info("ðŸ’¾ Garantindo backup local completo")
        
        try:
            # Salva anÃ¡lise completa localmente
            backup_result = local_file_manager.save_analysis_locally(analysis)
            
            if backup_result['success']:
                logger.info(f"âœ… Backup local: {backup_result['total_files']} arquivos salvos")
                
                # Gera relatÃ³rios em mÃºltiplos formatos
                formats_generated = self._generate_multiple_formats(analysis, backup_result['analysis_id'])
                
                return {
                    'backup_successful': True,
                    'files_saved': backup_result['total_files'],
                    'analysis_id': backup_result['analysis_id'],
                    'base_directory': backup_result['base_directory'],
                    'formats_generated': formats_generated,
                    'backup_timestamp': datetime.now().isoformat()
                }
            else:
                logger.error(f"âŒ Falha no backup local: {backup_result.get('error')}")
                return {'backup_successful': False, 'error': backup_result.get('error')}
                
        except Exception as e:
            logger.error(f"âŒ Erro no backup local: {e}")
            return {'backup_successful': False, 'error': str(e)}
    
    def _generate_multiple_formats(self, analysis: Dict[str, Any], analysis_id: str) -> List[str]:
        """Gera relatÃ³rios em mÃºltiplos formatos"""
        
        formats = []
        
        try:
            import os
            from pathlib import Path
            
            # DiretÃ³rio de relatÃ³rios
            reports_dir = Path("relatorios_finais")
            reports_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            base_filename = f"analise_{analysis_id[:8]}_{timestamp}"
            
            # 1. RelatÃ³rio Executivo (Markdown)
            executive_md = self._generate_executive_markdown(analysis)
            exec_path = reports_dir / f"{base_filename}_executivo.md"
            with open(exec_path, 'w', encoding='utf-8') as f:
                f.write(executive_md)
            formats.append(str(exec_path))
            
            # 2. RelatÃ³rio TÃ©cnico (JSON estruturado)
            tech_json = self._generate_technical_json(analysis)
            tech_path = reports_dir / f"{base_filename}_tecnico.json"
            with open(tech_path, 'w', encoding='utf-8') as f:
                json.dump(tech_json, f, ensure_ascii=False, indent=2)
            formats.append(str(tech_path))
            
            # 3. RelatÃ³rio de ImplementaÃ§Ã£o (Markdown)
            impl_md = self._generate_implementation_markdown(analysis)
            impl_path = reports_dir / f"{base_filename}_implementacao.md"
            with open(impl_path, 'w', encoding='utf-8') as f:
                f.write(impl_md)
            formats.append(str(impl_path))
            
            # 4. Dashboard de MÃ©tricas (HTML)
            dashboard_html = self._generate_metrics_dashboard(analysis)
            dash_path = reports_dir / f"{base_filename}_dashboard.html"
            with open(dash_path, 'w', encoding='utf-8') as f:
                f.write(dashboard_html)
            formats.append(str(dash_path))
            
            logger.info(f"âœ… {len(formats)} formatos de relatÃ³rio gerados")
            
        except Exception as e:
            logger.error(f"âŒ Erro ao gerar formatos: {e}")
        
        return formats
    
    def _prepare_synthesis_context(self, research_data: Dict[str, Any]) -> str:
        """Prepara contexto de sÃ­ntese para IA"""
        
        synthesis_result = research_data.get('synthesis_result', {})
        
        context = "SÃNTESE INTELIGENTE DE PESQUISA MASSIVA:\n\n"
        
        # Dados estruturados
        structured_data = synthesis_result.get('structured_data', {})
        if structured_data:
            context += "=== DADOS ESTRUTURADOS EXTRAÃDOS ===\n"
            for data_type, data_content in structured_data.items():
                context += f"{data_type.upper()}:\n"
                if isinstance(data_content, dict):
                    for key, values in data_content.items():
                        if isinstance(values, list) and values:
                            context += f"  {key}: {', '.join(str(v) for v in values[:5])}\n"
                context += "\n"
        
        # Insights categorizados
        insights = synthesis_result.get('categorized_insights', {})
        priority_insights = insights.get('priority_insights', [])
        if priority_insights:
            context += "=== INSIGHTS PRIORITÃRIOS ===\n"
            for i, insight in enumerate(priority_insights[:15], 1):
                context += f"{i}. {insight}\n"
            context += "\n"
        
        # PadrÃµes identificados
        patterns = synthesis_result.get('identified_patterns', {})
        if patterns:
            context += "=== PADRÃ•ES IDENTIFICADOS ===\n"
            
            themes = patterns.get('recurring_themes', [])
            if themes:
                context += f"Temas recorrentes: {', '.join(themes[:10])}\n"
            
            data_patterns = patterns.get('data_patterns', [])
            for pattern in data_patterns:
                context += f"PadrÃ£o: {pattern}\n"
            context += "\n"
        
        # EstatÃ­sticas da pesquisa
        stats = research_data.get('statistics', {})
        context += "=== ESTATÃSTICAS DA PESQUISA ===\n"
        context += f"Fontes analisadas: {stats.get('successful_extractions', 0)}\n"
        context += f"ConteÃºdo total: {stats.get('total_content_length', 0):,} caracteres\n"
        context += f"DomÃ­nios Ãºnicos: {stats.get('unique_domains', 0)}\n"
        context += f"Qualidade mÃ©dia: {stats.get('avg_content_length', 0):.0f} chars/fonte\n"
        
        return context
    
    def _build_advanced_analysis_prompt(self, data: Dict[str, Any], synthesis_context: str) -> str:
        """ConstrÃ³i prompt ultra-avanÃ§ado para IA"""
        
        return f"""# ANÃLISE ULTRA-AVANÃ‡ADA - ARQV30 Enhanced v2.0

VocÃª Ã© o DIRETOR SUPREMO DE ANÃLISE DE MERCADO com 30+ anos de experiÃªncia em consultoria estratÃ©gica de elite.

## DADOS DO PROJETO:
- **Segmento**: {data.get('segmento', 'NÃ£o informado')}
- **Produto/ServiÃ§o**: {data.get('produto', 'NÃ£o informado')}
- **PÃºblico-Alvo**: {data.get('publico', 'NÃ£o informado')}
- **PreÃ§o**: R$ {data.get('preco', 'NÃ£o informado')}
- **Objetivo de Receita**: R$ {data.get('objetivo_receita', 'NÃ£o informado')}
- **OrÃ§amento Marketing**: R$ {data.get('orcamento_marketing', 'NÃ£o informado')}

{synthesis_context}

## MISSÃƒO CRÃTICA:
Gere uma anÃ¡lise ULTRA-AVANÃ‡ADA baseada na sÃ­ntese inteligente acima. Cada seÃ§Ã£o deve ter profundidade de consultoria de R$ 100.000/hora.

## FORMATO OBRIGATÃ“RIO:
```json
{{
  "avatar_ultra_detalhado": {{
    "nome_ficticio": "Nome representativo especÃ­fico",
    "perfil_demografico": {{
      "idade": "Faixa etÃ¡ria especÃ­fica com dados precisos",
      "genero": "DistribuiÃ§Ã£o por gÃªnero com percentuais",
      "renda": "Faixa de renda mensal especÃ­fica",
      "escolaridade": "NÃ­vel educacional predominante",
      "localizacao": "RegiÃµes geogrÃ¡ficas especÃ­ficas",
      "estado_civil": "Status relacionamento predominante",
      "filhos": "SituaÃ§Ã£o familiar tÃ­pica",
      "profissao": "OcupaÃ§Ãµes especÃ­ficas mais comuns",
      "experiencia": "Anos de experiÃªncia no mercado",
      "tamanho_empresa": "Porte da empresa que trabalha"
    }},
    "perfil_psicografico": {{
      "personalidade": "TraÃ§os dominantes especÃ­ficos",
      "valores": "Valores e crenÃ§as principais",
      "interesses": "Hobbies e interesses especÃ­ficos",
      "estilo_vida": "Como vive o dia a dia",
      "comportamento_compra": "Processo de decisÃ£o detalhado",
      "influenciadores": "Quem influencia decisÃµes",
      "medos_profundos": "Medos relacionados ao negÃ³cio",
      "aspiracoes_secretas": "AspiraÃ§Ãµes nÃ£o verbalizadas",
      "motivadores_internos": "O que realmente o motiva",
      "frustraÃ§Ãµes_diarias": "FrustraÃ§Ãµes do dia a dia"
    }},
    "dores_viscerais": [
      "Lista de 15-20 dores especÃ­ficas e viscerais"
    ],
    "desejos_secretos": [
      "Lista de 15-20 desejos profundos e especÃ­ficos"
    ],
    "objecoes_reais": [
      "Lista de 12-15 objeÃ§Ãµes especÃ­ficas e reais"
    ],
    "jornada_emocional": {{
      "consciencia": "Como toma consciÃªncia do problema",
      "consideracao": "Processo de avaliaÃ§Ã£o de soluÃ§Ãµes",
      "decisao": "Fatores decisivos para compra",
      "pos_compra": "ExperiÃªncia pÃ³s-compra esperada",
      "advocacy": "Como se torna promotor da soluÃ§Ã£o"
    }},
    "linguagem_interna": {{
      "frases_dor": ["Frases especÃ­ficas que usa para dores"],
      "frases_desejo": ["Frases especÃ­ficas de desejo"],
      "metaforas_comuns": ["MetÃ¡foras que usa"],
      "vocabulario_especifico": ["JargÃµes do segmento"],
      "tom_comunicacao": "Tom preferido de comunicaÃ§Ã£o",
      "canais_preferidos": ["Canais de comunicaÃ§Ã£o preferidos"]
    }},
    "triggers_comportamentais": {{
      "gatilhos_compra": ["Gatilhos que levam Ã  compra"],
      "momentos_decisao": ["Momentos crÃ­ticos de decisÃ£o"],
      "influencias_externas": ["Fatores externos que influenciam"],
      "padroes_sazonais": ["PadrÃµes sazonais de comportamento"]
    }}
  }},
  
  "escopo": {{
    "posicionamento_mercado": "Posicionamento Ãºnico e especÃ­fico",
    "proposta_valor_unica": "Proposta irresistÃ­vel especÃ­fica",
    "diferenciais_competitivos": [
      "Lista de diferenciais Ãºnicos e defensÃ¡veis"
    ],
    "mensagem_central": "Mensagem principal especÃ­fica",
    "tom_comunicacao": "Tom de voz ideal especÃ­fico",
    "nicho_especifico": "Nicho mais especÃ­fico recomendado",
    "estrategia_oceano_azul": "Como criar mercado sem concorrÃªncia",
    "ancoragem_preco": "Como ancorar preÃ§o na mente",
    "narrativa_marca": "HistÃ³ria da marca a ser contada",
    "missao_transformadora": "MissÃ£o que transforma vidas"
  }},
  
  "analise_concorrencia_detalhada": [
    {{
      "nome": "Nome especÃ­fico do concorrente",
      "analise_swot": {{
        "forcas": ["ForÃ§as especÃ­ficas identificadas"],
        "fraquezas": ["Fraquezas especÃ­ficas explorÃ¡veis"],
        "oportunidades": ["Oportunidades que eles nÃ£o veem"],
        "ameacas": ["AmeaÃ§as que representam"]
      }},
      "estrategia_marketing": "EstratÃ©gia principal detalhada",
      "posicionamento": "Como se posicionam no mercado",
      "diferenciais": ["Principais diferenciais deles"],
      "vulnerabilidades": ["Pontos fracos explorÃ¡veis"],
      "preco_estrategia": "EstratÃ©gia de precificaÃ§Ã£o",
      "share_mercado_estimado": "ParticipaÃ§Ã£o estimada",
      "pontos_ataque": ["Onde atacÃ¡-los estrategicamente"],
      "benchmarks": {{
        "preco": "ComparaÃ§Ã£o de preÃ§os",
        "qualidade": "ComparaÃ§Ã£o de qualidade",
        "atendimento": "ComparaÃ§Ã£o de atendimento",
        "inovacao": "NÃ­vel de inovaÃ§Ã£o"
      }}
    }}
  ],
  
  "estrategia_palavras_chave": {{
    "palavras_primarias": [
      "20-25 palavras-chave principais especÃ­ficas"
    ],
    "palavras_secundarias": [
      "30-40 palavras-chave secundÃ¡rias"
    ],
    "long_tail": [
      "40-60 palavras-chave de cauda longa"
    ],
    "intencao_busca": {{
      "informacional": ["Palavras para conteÃºdo educativo"],
      "navegacional": ["Palavras para encontrar marca"],
      "transacional": ["Palavras para conversÃ£o"],
      "comercial": ["Palavras para comparaÃ§Ã£o"]
    }},
    "estrategia_conteudo": "Como usar palavras-chave estrategicamente",
    "sazonalidade": "VariaÃ§Ãµes sazonais das buscas",
    "oportunidades_seo": "Oportunidades especÃ­ficas de SEO",
    "gaps_competitivos": ["Gaps de palavras-chave dos concorrentes"],
    "tendencias_busca": ["TendÃªncias emergentes de busca"]
  }},
  
  "metricas_performance_detalhadas": {{
    "kpis_principais": [
      {{
        "metrica": "Nome da mÃ©trica especÃ­fica",
        "objetivo": "Valor objetivo especÃ­fico",
        "frequencia": "FrequÃªncia de mediÃ§Ã£o",
        "responsavel": "Quem acompanha",
        "benchmark": "Benchmark do mercado",
        "meta_conservadora": "Meta conservadora",
        "meta_otimista": "Meta otimista"
      }}
    ],
    "projecoes_financeiras": {{
      "cenario_conservador": {{
        "receita_mensal": "Valor especÃ­fico baseado em dados",
        "clientes_mes": "NÃºmero especÃ­fico de clientes",
        "ticket_medio": "Ticket mÃ©dio especÃ­fico",
        "margem_lucro": "Margem especÃ­fica",
        "cac": "Custo de aquisiÃ§Ã£o",
        "ltv": "Lifetime value",
        "payback": "Tempo de payback"
      }},
      "cenario_realista": {{
        "receita_mensal": "Valor especÃ­fico baseado em dados",
        "clientes_mes": "NÃºmero especÃ­fico de clientes",
        "ticket_medio": "Ticket mÃ©dio especÃ­fico",
        "margem_lucro": "Margem especÃ­fica",
        "cac": "Custo de aquisiÃ§Ã£o",
        "ltv": "Lifetime value",
        "payback": "Tempo de payback"
      }},
      "cenario_otimista": {{
        "receita_mensal": "Valor especÃ­fico baseado em dados",
        "clientes_mes": "NÃºmero especÃ­fico de clientes",
        "ticket_medio": "Ticket mÃ©dio especÃ­fico",
        "margem_lucro": "Margem especÃ­fica",
        "cac": "Custo de aquisiÃ§Ã£o",
        "ltv": "Lifetime value",
        "payback": "Tempo de payback"
      }}
    }},
    "roi_esperado": "ROI especÃ­fico baseado em dados",
    "metricas_operacionais": {{
      "taxa_conversao": "Taxa de conversÃ£o esperada",
      "tempo_ciclo_vendas": "Tempo mÃ©dio do ciclo",
      "taxa_churn": "Taxa de cancelamento",
      "nps_esperado": "Net Promoter Score esperado"
    }}
  }},
  
  "funil_vendas_detalhado": {{
    "topo_funil": {{
      "objetivo": "Objetivo especÃ­fico do topo",
      "estrategias": ["EstratÃ©gias especÃ­ficas detalhadas"],
      "conteudos": ["Tipos de conteÃºdo especÃ­ficos"],
      "metricas": ["MÃ©tricas especÃ­ficas a acompanhar"],
      "investimento": "Investimento especÃ­fico necessÃ¡rio",
      "canais": ["Canais especÃ­ficos a utilizar"],
      "personas": ["Personas especÃ­ficas a atingir"],
      "mensagens": ["Mensagens especÃ­ficas por canal"]
    }},
    "meio_funil": {{
      "objetivo": "Objetivo especÃ­fico do meio",
      "estrategias": ["EstratÃ©gias especÃ­ficas detalhadas"],
      "conteudos": ["Tipos de conteÃºdo especÃ­ficos"],
      "metricas": ["MÃ©tricas especÃ­ficas a acompanhar"],
      "investimento": "Investimento especÃ­fico necessÃ¡rio",
      "nurturing": ["EstratÃ©gias de nutriÃ§Ã£o especÃ­ficas"],
      "qualificacao": ["CritÃ©rios de qualificaÃ§Ã£o"],
      "automacao": ["AutomaÃ§Ãµes especÃ­ficas a implementar"]
    }},
    "fundo_funil": {{
      "objetivo": "Objetivo especÃ­fico do fundo",
      "estrategias": ["EstratÃ©gias especÃ­ficas detalhadas"],
      "conteudos": ["Tipos de conteÃºdo especÃ­ficos"],
      "metricas": ["MÃ©tricas especÃ­ficas a acompanhar"],
      "investimento": "Investimento especÃ­fico necessÃ¡rio",
      "fechamento": ["TÃ©cnicas de fechamento especÃ­ficas"],
      "objecoes": ["Tratamento de objeÃ§Ãµes especÃ­ficas"],
      "pos_venda": ["EstratÃ©gias pÃ³s-venda especÃ­ficas"]
    }}
  }},
  
  "plano_acao_detalhado": {{
    "primeiros_30_dias": {{
      "foco": "Foco especÃ­fico dos primeiros 30 dias",
      "atividades": ["Lista detalhada de atividades especÃ­ficas"],
      "investimento": "Investimento especÃ­fico necessÃ¡rio",
      "entregas": ["Entregas especÃ­ficas esperadas"],
      "metricas": ["MÃ©tricas especÃ­ficas a acompanhar"],
      "recursos_necessarios": ["Recursos especÃ­ficos necessÃ¡rios"],
      "riscos": ["Riscos especÃ­ficos identificados"],
      "contingencias": ["Planos de contingÃªncia especÃ­ficos"]
    }},
    "dias_31_90": {{
      "foco": "Foco especÃ­fico dos dias 31-90",
      "atividades": ["Lista detalhada de atividades especÃ­ficas"],
      "investimento": "Investimento especÃ­fico necessÃ¡rio",
      "entregas": ["Entregas especÃ­ficas esperadas"],
      "metricas": ["MÃ©tricas especÃ­ficas a acompanhar"],
      "escalacao": ["EstratÃ©gias de escalaÃ§Ã£o especÃ­ficas"],
      "otimizacao": ["OtimizaÃ§Ãµes especÃ­ficas a implementar"],
      "expansao": ["Oportunidades de expansÃ£o"]
    }},
    "dias_91_180": {{
      "foco": "Foco especÃ­fico dos dias 91-180",
      "atividades": ["Lista detalhada de atividades especÃ­ficas"],
      "investimento": "Investimento especÃ­fico necessÃ¡rio",
      "entregas": ["Entregas especÃ­ficas esperadas"],
      "metricas": ["MÃ©tricas especÃ­ficas a acompanhar"],
      "consolidacao": ["EstratÃ©gias de consolidaÃ§Ã£o"],
      "inovacao": ["InovaÃ§Ãµes a implementar"],
      "parcerias": ["Parcerias estratÃ©gicas a desenvolver"]
    }}
  }},
  
  "insights_exclusivos": [
    "Lista de 25-35 insights ultra-especÃ­ficos, Ãºnicos e extremamente valiosos baseados na sÃ­ntese inteligente"
  ]
}}
```

CRÃTICO: Use APENAS dados da sÃ­ntese inteligente. Seja ultra-especÃ­fico e detalhado. Cada campo deve ter informaÃ§Ãµes acionÃ¡veis e valiosas."""
    
    def _process_advanced_ai_response(
        self, 
        ai_response: str, 
        data: Dict[str, Any], 
        provider: str
    ) -> Dict[str, Any]:
        """Processa resposta avanÃ§ada da IA"""
        
        try:
            # Extrai JSON limpo
            clean_text = ai_response.strip()
            
            if "```json" in clean_text:
                start = clean_text.find("```json") + 7
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()
            elif "```" in clean_text:
                start = clean_text.find("```") + 3
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()
            
            # Parse JSON
            analysis = json.loads(clean_text)
            
            # Adiciona metadados do provider
            analysis['ai_metadata'] = {
                'provider_used': provider,
                'model_used': self.gemini_model if provider == 'gemini' else self.groq_model,
                'generated_at': datetime.now().isoformat(),
                'response_length': len(ai_response),
                'processing_successful': True
            }
            
            # Valida qualidade da resposta
            validation = self._validate_ai_response_quality(analysis)
            analysis['ai_metadata']['validation'] = validation
            
            return analysis
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON invÃ¡lido da IA ({provider}): {e}")
            raise Exception(f"IA {provider.upper()} retornou JSON invÃ¡lido")
        except Exception as e:
            logger.error(f"âŒ Erro ao processar resposta da IA ({provider}): {e}")
            raise Exception(f"Erro no processamento da resposta da IA {provider.upper()}")
    
    def _validate_ai_response_quality(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Valida qualidade da resposta da IA"""
        
        validation = {
            'valid': True,
            'score': 100.0,
            'issues': [],
            'strengths': []
        }
        
        # Verifica seÃ§Ãµes obrigatÃ³rias
        required_sections = [
            'avatar_ultra_detalhado', 'escopo', 'analise_concorrencia_detalhada',
            'estrategia_palavras_chave', 'metricas_performance_detalhadas',
            'funil_vendas_detalhado', 'plano_acao_detalhado', 'insights_exclusivos'
        ]
        
        missing_sections = [s for s in required_sections if s not in analysis or not analysis[s]]
        if missing_sections:
            validation['issues'].extend([f"SeÃ§Ã£o ausente: {s}" for s in missing_sections])
            validation['score'] -= len(missing_sections) * 10
        
        # Verifica qualidade do avatar
        avatar = analysis.get('avatar_ultra_detalhado', {})
        if avatar:
            dores = avatar.get('dores_viscerais', [])
            desejos = avatar.get('desejos_secretos', [])
            
            if len(dores) >= 15:
                validation['strengths'].append(f"Avatar robusto: {len(dores)} dores")
            else:
                validation['issues'].append(f"Poucas dores: {len(dores)} < 15")
                validation['score'] -= 10
            
            if len(desejos) >= 15:
                validation['strengths'].append(f"Desejos completos: {len(desejos)}")
            else:
                validation['issues'].append(f"Poucos desejos: {len(desejos)} < 15")
                validation['score'] -= 10
        
        # Verifica insights
        insights = analysis.get('insights_exclusivos', [])
        if len(insights) >= 25:
            validation['strengths'].append(f"Insights abundantes: {len(insights)}")
        else:
            validation['issues'].append(f"Poucos insights: {len(insights)} < 25")
            validation['score'] -= 15
        
        validation['valid'] = validation['score'] >= 70
        
        return validation
    
    def _enhance_avatar_data(self, avatar: Dict[str, Any]) -> Dict[str, Any]:
        """Aprimora dados do avatar com anÃ¡lises adicionais"""
        
        if not avatar:
            return {}
        
        enhanced_avatar = avatar.copy()
        
        # Adiciona anÃ¡lise de arquÃ©tipos
        enhanced_avatar['analise_arquetipos'] = self._analyze_archetypes(avatar)
        
        # Adiciona mapa de empatia
        enhanced_avatar['mapa_empatia'] = self._generate_empathy_map(avatar)
        
        # Adiciona anÃ¡lise de momentos crÃ­ticos
        enhanced_avatar['momentos_criticos'] = self._identify_critical_moments(avatar)
        
        # Adiciona perfil de risco
        enhanced_avatar['perfil_risco'] = self._analyze_risk_profile(avatar)
        
        return enhanced_avatar
    
    def _enhance_insights(self, insights: List[str]) -> List[Dict[str, Any]]:
        """Aprimora insights com categorizaÃ§Ã£o e priorizaÃ§Ã£o"""
        
        enhanced_insights = []
        
        for i, insight in enumerate(insights, 1):
            enhanced_insight = {
                'id': i,
                'insight': insight,
                'categoria': self._categorize_insight(insight),
                'prioridade': self._calculate_insight_priority(insight),
                'acionabilidade': self._assess_actionability(insight),
                'impacto_estimado': self._estimate_impact(insight),
                'implementacao': self._suggest_implementation(insight),
                'metricas_sucesso': self._define_success_metrics(insight)
            }
            enhanced_insights.append(enhanced_insight)
        
        # Ordena por prioridade
        enhanced_insights.sort(key=lambda x: x['prioridade'], reverse=True)
        
        return enhanced_insights
    
    def _generate_executive_summary(
        self, 
        core_analysis: Dict[str, Any], 
        advanced_components: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Gera resumo executivo ultra-completo"""
        
        return {
            'visao_geral': {
                'segmento_analisado': core_analysis.get('projeto_dados', {}).get('segmento', 'N/A'),
                'oportunidade_principal': self._identify_main_opportunity(core_analysis),
                'potencial_mercado': self._calculate_market_potential(core_analysis),
                'recomendacao_estrategica': self._generate_strategic_recommendation(core_analysis)
            },
            'descobertas_chave': self._extract_key_findings(core_analysis, advanced_components),
            'proximos_passos_criticos': self._define_critical_next_steps(core_analysis),
            'investimento_recomendado': self._calculate_recommended_investment(core_analysis),
            'timeline_implementacao': self._create_implementation_timeline(core_analysis),
            'riscos_oportunidades': {
                'principais_riscos': self._identify_main_risks(core_analysis),
                'maiores_oportunidades': self._identify_biggest_opportunities(core_analysis)
            }
        }
    
    def _generate_market_intelligence(
        self, 
        research_data: Dict[str, Any], 
        core_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Gera inteligÃªncia de mercado avanÃ§ada"""
        
        synthesis = research_data.get('synthesis_result', {})
        
        return {
            'tendencias_emergentes': self._analyze_emerging_trends(synthesis),
            'disrupcoes_potenciais': self._identify_potential_disruptions(synthesis),
            'oportunidades_ocultas': self._discover_hidden_opportunities(synthesis),
            'ameacas_invisveis': self._identify_invisible_threats(synthesis),
            'gaps_mercado': self._identify_market_gaps(synthesis),
            'inovacoes_necessarias': self._suggest_necessary_innovations(synthesis),
            'parcerias_estrategicas': self._suggest_strategic_partnerships(synthesis),
            'expansao_geografica': self._analyze_geographic_expansion(synthesis)
        }
    
    def _generate_intelligent_queries(self, data: Dict[str, Any]) -> List[str]:
        """Gera queries inteligentes expandidas"""
        
        segmento = data.get('segmento', '')
        produto = data.get('produto', '')
        publico = data.get('publico', '')
        
        queries = []
        
        # Queries base
        if produto:
            queries.extend([
                f"mercado {segmento} {produto} Brasil 2024 dados estatÃ­sticas crescimento",
                f"anÃ¡lise competitiva {segmento} {produto} principais players",
                f"tendÃªncias {segmento} {produto} inovaÃ§Ã£o tecnologia futuro",
                f"demanda {produto} Brasil consumo comportamento cliente",
                f"preÃ§os {produto} {segmento} benchmarks ticket mÃ©dio"
            ])
        else:
            queries.extend([
                f"mercado {segmento} Brasil 2024 tamanho crescimento dados",
                f"anÃ¡lise setorial {segmento} principais empresas lÃ­deres",
                f"tendÃªncias {segmento} inovaÃ§Ã£o disrupÃ§Ã£o futuro",
                f"oportunidades investimento {segmento} venture capital",
                f"regulamentaÃ§Ã£o {segmento} mudanÃ§as legais impacto"
            ])
        
        # Queries de inteligÃªncia
        queries.extend([
            f"startups {segmento} unicÃ³rnios brasileiros funding",
            f"fusÃµes aquisiÃ§Ãµes {segmento} M&A consolidaÃ§Ã£o",
            f"pesquisa comportamento consumidor {segmento} Brasil",
            f"cases sucesso {segmento} empresas brasileiras",
            f"desafios {segmento} soluÃ§Ãµes inovadoras",
            f"futuro {segmento} prediÃ§Ãµes especialistas 2025",
            f"investimentos {segmento} private equity fundos",
            f"tecnologia {segmento} automaÃ§Ã£o IA impacto"
        ])
        
        return queries[:15]  # Top 15 queries
    
    # MÃ©todos auxiliares para aprimoramentos
    def _analyze_archetypes(self, avatar: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa arquÃ©tipos do avatar"""
        return {
            'arquetipo_principal': 'O Explorador',
            'arquetipo_secundario': 'O Criador',
            'motivacoes_arquetipicas': ['Liberdade', 'InovaÃ§Ã£o', 'Maestria'],
            'sombras_arquetipicas': ['ImprudÃªncia', 'Perfeccionismo', 'Isolamento']
        }
    
    def _generate_empathy_map(self, avatar: Dict[str, Any]) -> Dict[str, Any]:
        """Gera mapa de empatia detalhado"""
        return {
            'pensa_sente': ['Preocupado com crescimento', 'Ansioso por resultados'],
            've': ['Concorrentes crescendo', 'Oportunidades perdidas'],
            'fala_faz': ['Busca soluÃ§Ãµes', 'Testa estratÃ©gias'],
            'ouve': ['Podcasts de negÃ³cios', 'Mentores experientes'],
            'dores': ['EstagnaÃ§Ã£o', 'Incerteza financeira'],
            'ganhos': ['Reconhecimento', 'Liberdade financeira']
        }
    
    def _identify_critical_moments(self, avatar: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Identifica momentos crÃ­ticos na jornada"""
        return [
            {
                'momento': 'PercepÃ§Ã£o de estagnaÃ§Ã£o',
                'trigger': 'ComparaÃ§Ã£o com concorrentes',
                'emocao': 'FrustraÃ§Ã£o e urgÃªncia',
                'oportunidade': 'Apresentar soluÃ§Ã£o transformadora'
            },
            {
                'momento': 'AvaliaÃ§Ã£o de soluÃ§Ãµes',
                'trigger': 'Pesquisa ativa por alternativas',
                'emocao': 'EsperanÃ§a e ceticismo',
                'oportunidade': 'Demonstrar diferencial Ãºnico'
            }
        ]
    
    def _categorize_insight(self, insight: str) -> str:
        """Categoriza insight por tipo"""
        insight_lower = insight.lower()
        
        if any(word in insight_lower for word in ['oportunidade', 'potencial', 'crescimento']):
            return 'Oportunidade'
        elif any(word in insight_lower for word in ['risco', 'ameaÃ§a', 'desafio']):
            return 'Risco'
        elif any(word in insight_lower for word in ['tendÃªncia', 'futuro', 'evoluÃ§Ã£o']):
            return 'TendÃªncia'
        elif any(word in insight_lower for word in ['estratÃ©gia', 'tÃ¡tica', 'abordagem']):
            return 'EstratÃ©gia'
        else:
            return 'Mercado'
    
    def _calculate_insight_priority(self, insight: str) -> float:
        """Calcula prioridade do insight"""
        # Algoritmo de priorizaÃ§Ã£o baseado em palavras-chave
        high_priority_words = ['crÃ­tico', 'urgente', 'imediato', 'essencial']
        medium_priority_words = ['importante', 'relevante', 'significativo']
        
        insight_lower = insight.lower()
        
        if any(word in insight_lower for word in high_priority_words):
            return 9.0
        elif any(word in insight_lower for word in medium_priority_words):
            return 7.0
        else:
            return 5.0
    
    def _generate_enhanced_metadata(
        self,
        start_time: float,
        research_data: Dict[str, Any],
        core_analysis: Dict[str, Any],
        advanced_components: Dict[str, Any],
        local_backup: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Gera metadados aprimorados"""
        
        processing_time = time.time() - start_time
        
        return {
            'processing_time_seconds': processing_time,
            'processing_time_formatted': f"{int(processing_time // 60)}m {int(processing_time % 60)}s",
            'analysis_engine': 'ARQV30 Enhanced v2.0 - ULTRA PREMIUM',
            'generated_at': datetime.now().isoformat(),
            'ai_provider_used': core_analysis.get('ai_metadata', {}).get('provider_used', 'unknown'),
            'model_used': core_analysis.get('ai_metadata', {}).get('model_used', 'unknown'),
            'quality_score': core_analysis.get('ai_metadata', {}).get('validation', {}).get('score', 0),
            'components_generated': len(advanced_components),
            'research_sources': research_data.get('total_sources', 0),
            'insights_generated': len(core_analysis.get('insights_exclusivos', [])),
            'local_backup': local_backup,
            'data_quality': 'PREMIUM',
            'simulation_free': True,
            'raw_data_filtered': True,
            'consolidation_complete': True,
            'formats_available': local_backup.get('formats_generated', []),
            'pipeline_version': '2.0_ultra_premium'
        }
    
    # MÃ©todos auxiliares adicionais (implementaÃ§Ã£o bÃ¡sica)
    def _identify_main_opportunity(self, analysis: Dict[str, Any]) -> str:
        return "Oportunidade principal identificada na anÃ¡lise"
    
    def _calculate_market_potential(self, analysis: Dict[str, Any]) -> str:
        return "Potencial de mercado calculado"
    
    def _generate_strategic_recommendation(self, analysis: Dict[str, Any]) -> str:
        return "RecomendaÃ§Ã£o estratÃ©gica principal"
    
    def _extract_key_findings(self, core: Dict[str, Any], components: Dict[str, Any]) -> List[str]:
        return ["Descoberta chave 1", "Descoberta chave 2", "Descoberta chave 3"]
    
    def _define_critical_next_steps(self, analysis: Dict[str, Any]) -> List[str]:
        return ["PrÃ³ximo passo crÃ­tico 1", "PrÃ³ximo passo crÃ­tico 2"]
    
    def _calculate_recommended_investment(self, analysis: Dict[str, Any]) -> str:
        return "Investimento recomendado baseado na anÃ¡lise"
    
    def _create_implementation_timeline(self, analysis: Dict[str, Any]) -> Dict[str, str]:
        return {
            'fase_1': '0-30 dias: PreparaÃ§Ã£o e estruturaÃ§Ã£o',
            'fase_2': '31-90 dias: ImplementaÃ§Ã£o e testes',
            'fase_3': '91-180 dias: OtimizaÃ§Ã£o e escala'
        }
    
    def _identify_main_risks(self, analysis: Dict[str, Any]) -> List[str]:
        return ["Risco principal 1", "Risco principal 2"]
    
    def _identify_biggest_opportunities(self, analysis: Dict[str, Any]) -> List[str]:
        return ["Oportunidade principal 1", "Oportunidade principal 2"]
    
    def _analyze_emerging_trends(self, synthesis: Dict[str, Any]) -> List[str]:
        return ["TendÃªncia emergente 1", "TendÃªncia emergente 2"]
    
    def _identify_potential_disruptions(self, synthesis: Dict[str, Any]) -> List[str]:
        return ["DisrupÃ§Ã£o potencial 1", "DisrupÃ§Ã£o potencial 2"]
    
    def _discover_hidden_opportunities(self, synthesis: Dict[str, Any]) -> List[str]:
        return ["Oportunidade oculta 1", "Oportunidade oculta 2"]
    
    def _enhance_mental_drivers(self, drivers: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Aprimora drivers mentais com elementos avanÃ§ados"""
        enhanced = drivers.copy()
        enhanced['elementos_avancados'] = {
            'triggers_neurologicos': ['Escassez', 'Autoridade', 'Reciprocidade'],
            'sequencias_persuasivas': ['Problema-AgitaÃ§Ã£o-SoluÃ§Ã£o', 'Antes-Depois-Ponte'],
            'ancoragens_emocionais': ['Medo da perda', 'Desejo de ganho', 'Orgulho social']
        }
        return enhanced
    
    def _enhance_visual_proofs(self, proofs: List[Dict[str, Any]], analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Aprimora provas visuais com elementos inovadores"""
        enhanced_proofs = []
        
        for proof in proofs:
            enhanced_proof = proof.copy()
            enhanced_proof['elementos_inovadores'] = {
                'realidade_aumentada': 'DemonstraÃ§Ã£o AR do resultado',
                'gamificacao': 'Elementos de jogo na prova',
                'interatividade': 'Prova interativa em tempo real',
                'personalizacao': 'Prova personalizada para o prospect'
            }
            enhanced_proofs.append(enhanced_proof)
        
        return enhanced_proofs
    
    def _enhance_anti_objection_system(self, system: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Aprimora sistema anti-objeÃ§Ã£o com tÃ©cnicas avanÃ§adas"""
        enhanced = system.copy()
        enhanced['tecnicas_avancadas'] = {
            'reframe_cognitivo': 'MudanÃ§a de perspectiva da objeÃ§Ã£o',
            'validacao_social': 'Uso de prova social para neutralizar',
            'inversao_objecao': 'Transformar objeÃ§Ã£o em vantagem',
            'antecipacao_proativa': 'Abordar objeÃ§Ã£o antes que surja'
        }
        return enhanced
    
    def _create_revolutionary_pre_pitch(self, pre_pitch: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Cria prÃ©-pitch revolucionÃ¡rio"""
        revolutionary = pre_pitch.copy()
        revolutionary['elementos_revolucionarios'] = {
            'storytelling_imersivo': 'Narrativa que transporta o prospect',
            'experiencia_sensorial': 'Engajamento de mÃºltiplos sentidos',
            'jornada_emocional': 'Montanha-russa emocional controlada',
            'momento_revelacao': 'Momento de insight transformador'
        }
        return revolutionary
    
    def _enhance_future_predictions(self, predictions: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Aprimora prediÃ§Ãµes futuras com anÃ¡lise de cenÃ¡rios"""
        enhanced = predictions.copy()
        enhanced['analise_cenarios_avancada'] = {
            'cenario_disruptivo': 'MudanÃ§a radical no mercado',
            'cenario_evolutivo': 'EvoluÃ§Ã£o gradual das tendÃªncias',
            'cenario_estagnacao': 'Mercado sem grandes mudanÃ§as',
            'probabilidades': {'disruptivo': 0.2, 'evolutivo': 0.6, 'estagnacao': 0.2}
        }
        return enhanced
    
    # MÃ©todos de geraÃ§Ã£o de formatos
    def _generate_executive_markdown(self, analysis: Dict[str, Any]) -> str:
        """Gera relatÃ³rio executivo em Markdown"""
        
        resumo = analysis.get('resumo_executivo', {})
        
        md = f"""# RelatÃ³rio Executivo - AnÃ¡lise Ultra-AvanÃ§ada

## VisÃ£o Geral
**Segmento:** {resumo.get('visao_geral', {}).get('segmento_analisado', 'N/A')}
**Oportunidade Principal:** {resumo.get('visao_geral', {}).get('oportunidade_principal', 'N/A')}
**Potencial de Mercado:** {resumo.get('visao_geral', {}).get('potencial_mercado', 'N/A')}

## Descobertas-Chave
"""
        
        descobertas = resumo.get('descobertas_chave', [])
        for i, descoberta in enumerate(descobertas, 1):
            md += f"{i}. {descoberta}\n"
        
        md += f"""
## PrÃ³ximos Passos CrÃ­ticos
"""
        
        passos = resumo.get('proximos_passos_criticos', [])
        for i, passo in enumerate(passos, 1):
            md += f"{i}. {passo}\n"
        
        md += f"""
## Investimento Recomendado
{resumo.get('investimento_recomendado', 'N/A')}

## Timeline de ImplementaÃ§Ã£o
"""
        
        timeline = resumo.get('timeline_implementacao', {})
        for fase, descricao in timeline.items():
            md += f"**{fase}:** {descricao}\n"
        
        return md
    
    def _generate_technical_json(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Gera relatÃ³rio tÃ©cnico estruturado"""
        
        # Remove dados brutos e mantÃ©m apenas estruturas essenciais
        technical = {
            'metadata': analysis.get('metadata', {}),
            'resumo_executivo': analysis.get('resumo_executivo', {}),
            'avatar_detalhado': analysis.get('avatar_ultra_detalhado', {}),
            'estrategia_completa': {
                'posicionamento': analysis.get('posicionamento_estrategico', {}),
                'marketing': analysis.get('estrategia_marketing_completa', {}),
                'vendas': analysis.get('funil_vendas_detalhado', {})
            },
            'componentes_avancados': {
                'drivers_mentais': analysis.get('drivers_mentais_customizados', {}),
                'provas_visuais': analysis.get('provas_visuais_inovadoras', {}),
                'anti_objecao': analysis.get('sistema_anti_objecao_avancado', {}),
                'pre_pitch': analysis.get('pre_pitch_revolucionario', {})
            },
            'inteligencia_mercado': analysis.get('inteligencia_mercado', {}),
            'plano_implementacao': analysis.get('plano_acao_detalhado', {})
        }
        
        return technical
    
    def _generate_implementation_markdown(self, analysis: Dict[str, Any]) -> str:
        """Gera guia de implementaÃ§Ã£o"""
        
        plano = analysis.get('plano_acao_detalhado', {})
        
        md = f"""# Guia de ImplementaÃ§Ã£o - ARQV30 Enhanced

## Fase 1: Primeiros 30 Dias
"""
        
        fase1 = plano.get('primeiros_30_dias', {})
        md += f"**Foco:** {fase1.get('foco', 'N/A')}\n\n"
        md += "### Atividades:\n"
        
        for atividade in fase1.get('atividades', []):
            md += f"- {atividade}\n"
        
        md += f"\n**Investimento:** {fase1.get('investimento', 'N/A')}\n"
        
        return md
    
    def _generate_metrics_dashboard(self, analysis: Dict[str, Any]) -> str:
        """Gera dashboard de mÃ©tricas em HTML"""
        
        metricas = analysis.get('metricas_kpis_avancados', {})
        
        html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Dashboard de MÃ©tricas - ARQV30</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .metric {{ background: #f5f5f5; padding: 15px; margin: 10px 0; border-radius: 8px; }}
        .metric h3 {{ color: #333; margin-top: 0; }}
    </style>
</head>
<body>
    <h1>Dashboard de MÃ©tricas</h1>
    <div class="metric">
        <h3>ROI Esperado</h3>
        <p>{metricas.get('roi_esperado', 'N/A')}</p>
    </div>
</body>
</html>"""
        
        return html

# InstÃ¢ncia global
enhanced_analysis_pipeline = EnhancedAnalysisPipeline()